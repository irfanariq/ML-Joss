{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas Besar I\n",
    "--------------------------\n",
    "Kelompok:\n",
    "\n",
    "- Diki Ardian Wirasandi (13515092)\n",
    "- Irfan Ariq (13515112)\n",
    "- Pratamamia Agung Prihatmaja (13515142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def purity(cluster_pred, label):\n",
    "    outlier = False\n",
    "    size_data = len(cluster_pred)\n",
    "    \n",
    "    data_per_cluster = [[] for i in range(len(set(cluster_pred)))]\n",
    "    for i,x in enumerate(cluster_pred):\n",
    "        if x == -1:\n",
    "            outlier = True\n",
    "        data_per_cluster[x].append(label[i])\n",
    "\n",
    "    sum = 0\n",
    "    for clust in data_per_cluster:\n",
    "        sum += stats.mode(clust)[1][0]\n",
    "    \n",
    "    if outlier:\n",
    "        sum -= stats.mode(clust)[1][0]\n",
    "        size_data -= len(clust)\n",
    "\n",
    "    return sum/size_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative Clustering\n",
    "------------------------------------------\n",
    "*Agglomerative clustering* merupakan suatu teknik dalam melakukan *clustering* yang menggunakan pendekatan *hierarchical*. Ide utama dari teknik ini adalah dengan melakukan penggabungan dua buah *cluster* dengan jarak terdekat pada setiap iterasi hingga diperoleh banyak *cluster* sesuai yang dikehendaki.\n",
    "\n",
    "Dalam notasi *pseudocode*, algoritma *agglomerative clustering* adalah sebagai berikut.\n",
    "\n",
    "```\n",
    "WHILE (length(clusters_now) > nb_target_cluster) DO\n",
    "    pair_merged = get_shortest_distance_pair(clusters_now)\n",
    "    merge_cluster(pair_merged)\n",
    "    update_cluster(clusters_now)\n",
    "```\n",
    "\n",
    "Terdapat beberapa pendekatan dalam menentukan pasangan *cluster* dengan jarak terdekat. Pendekatan tersebut adalah sebagai berikut.\n",
    "\n",
    "1. ***Complete linkage***. Dilakukan dengan menghitung jarak antara dua titik terjauh pada kedua *cluster*.\n",
    "![complete linkage](img/complete.png \"Complete linkage\")\n",
    "\n",
    "2. ***Single linkage***. Dilakukan dengan menghitung jarak antara dua titik terdekat pada kedua *cluster*.\n",
    "![single linkage](img/single.png \"Single linkage\")\n",
    "\n",
    "3. ***Average linkage***. Dilakukan dengan menghitung jarak rata-rata antara semua pasangan titik pada kedua cluster.\n",
    "![average linkage](img/average.png \"Average linkage\")\n",
    "\n",
    "4. ***Average-group linkage***. Dilakukan dengan menghitung jarak antara *centroid* dari kedua *cluster*.\n",
    "![average group linkage](img/average_group.png \"Average-group linkage\")\n",
    "\n",
    "Teknik penghitungan jarak juga beragam. Dalam implementasi ini, diterapkan jarak *manhattan*, *euclidean*, dan *cosine*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class AgglomerativeClustering:\n",
    "    '''\n",
    "    Kelas untuk mengakomodasi nilai metode agglomerative clustering\n",
    "    '''\n",
    "    \n",
    "    # Nilai default parameter\n",
    "    n_clusters = 2\n",
    "    linkage = 'complete'\n",
    "    metrics = 'euclidean'\n",
    "    \n",
    "    available_metrics = ['euclidean', 'manhattan', 'cosine']\n",
    "    available_linkage = ['complete', 'single', 'average_group', 'average']\n",
    "    \n",
    "    def __init__(self, n_clusters=n_clusters, linkage=linkage, metrics=metrics):\n",
    "        '''\n",
    "        Inisiasi kelas. Parameter yang dibutuhkan untuk setiap kelas diinisiasi atau diisi dengan nilai default \n",
    "        '''\n",
    "        if n_clusters <= 0:\n",
    "            raise Exception('n_clusters must be higher than 0')\n",
    "        if metrics not in self.available_metrics:\n",
    "            raise Exception('No metrics \\'' + str(metrics) + '\\'. Available metrics '+ str(self.available_metrics))\n",
    "        if linkage not in self.available_linkage:\n",
    "            raise Exception('No linkage \\'' + str(linkage) + '\\'. Available linkage '+ str(self.available_linkage))\n",
    "        self.metrics = metrics\n",
    "        self.n_clusters = n_clusters\n",
    "        self.linkage = linkage\n",
    "        \n",
    "    def __euclidean_distance(self, data1, data2):\n",
    "        '''\n",
    "        Fungsi untuk menghitung euclidean distance di antara dua vector dengan panjang yang sama\n",
    "        '''\n",
    "        sum = 0\n",
    "        if (len(data1) == len(data2)):\n",
    "            for x1, x2 in zip(data1, data2):\n",
    "                sum += (x1 - x2)**2\n",
    "            dist = math.sqrt(sum)\n",
    "            return dist\n",
    "        else:\n",
    "            raise Exception('Length doesn\\'t match')\n",
    "\n",
    "    def __manhattan_distance(self, data1, data2):\n",
    "        '''\n",
    "        Fungsi untuk menghitung manhattan distance di antara dua vector dengan panjang yang sama\n",
    "        '''\n",
    "        sum = 0\n",
    "        if (len(data1) == len(data2)):\n",
    "            for x1, x2 in zip(data1, data2):\n",
    "                sum += abs(x1 - x2)\n",
    "            return sum\n",
    "        else:\n",
    "            raise Exception('Length doesn\\'t match')\n",
    "            \n",
    "    def __cosine_distance(self, data1, data2):\n",
    "        '''\n",
    "        Fungsi untuk menghitung cosine distance di antara dua vector dengan panjang yang sama\n",
    "        '''\n",
    "        return np.dot(data1, data2) / (np.linalg.norm(data1) * np.linalg.norm(data2))\n",
    "\n",
    "    def __get_distance(self, data1, data2, metrics):\n",
    "        '''\n",
    "        Fungsi untuk menghitung jarak dua vector dengan metric pengukuran jarak yang telah ditentukan\n",
    "        '''\n",
    "        if (metrics == 'euclidean'):\n",
    "            dist = self.__euclidean_distance(data1, data2)\n",
    "        elif (metrics == 'manhattan'):\n",
    "            dist = self.__manhattan_distance(data1, data2)\n",
    "        elif (metrics == 'cosine'):\n",
    "            dist = self.__cosine_distance(data1, data2)\n",
    "        else:\n",
    "            raise Exception('Metrics not defined')\n",
    "        return dist\n",
    "    \n",
    "    def __complete_linkage(self, cluster1, cluster2, dist_matrix):\n",
    "        '''\n",
    "        Fungsi untuk menghitung jarak antara dua cluster dengan pendekatan complete linkage\n",
    "        '''\n",
    "        max_dist = 0\n",
    "        for v1 in cluster1:\n",
    "            for v2 in cluster2:\n",
    "                if (max_dist < dist_matrix[v1][v2]):\n",
    "                    max_dist = dist_matrix[v1][v2]\n",
    "        return max_dist\n",
    "\n",
    "    def __single_linkage(self, cluster1, cluster2, dist_matrix):\n",
    "        '''\n",
    "        Fungsi untuk menghitung jarak antara dua cluster dengan pendekatan single linkage\n",
    "        '''\n",
    "        min_dist = None\n",
    "        for v1 in cluster1:\n",
    "            for v2 in cluster2:\n",
    "                if (min_dist is None) or (min_dist > dist_matrix[v1][v2]):\n",
    "                    min_dist = dist_matrix[v1][v2]\n",
    "        return min_dist\n",
    "\n",
    "    def __average_linkage(self, cluster1, cluster2, dist_matrix):\n",
    "        '''\n",
    "        Fungsi untuk menghitung jarak antara dua cluster dengan pendekatan average linkage\n",
    "        '''\n",
    "        sum_dist = 0\n",
    "        count_dist = 0\n",
    "        for v1 in cluster1:\n",
    "            for v2 in cluster2:\n",
    "                sum_dist += dist_matrix[v1][v2]\n",
    "                count_dist += 1\n",
    "        return float(sum_dist)/float(count_dist)\n",
    "\n",
    "    def __group_average_linkage(self, cluster1, cluster2, data, distance):\n",
    "        '''\n",
    "        Fungsi untuk menghitung jarak antara dua cluster dengan pendekatan average group linkage\n",
    "        '''\n",
    "        data1 = [data[i] for i in cluster1]\n",
    "        data2 = [data[i] for i in cluster2]\n",
    "\n",
    "        avg1 = np.mean(data1, axis = 0)\n",
    "        avg2 = np.mean(data2, axis = 0)\n",
    "\n",
    "        return self.__get_distance(avg1, avg2, distance)\n",
    "    \n",
    "    def __calculate_distance_matrix(self, data, metrics):\n",
    "        '''\n",
    "        Fungsi untuk menghitung distance matrix untuk semua pasangan vector di dalam data\n",
    "        '''\n",
    "        dist_matrix = []\n",
    "        for idx1, data1 in enumerate(data):\n",
    "            curr_dist_matrix = []\n",
    "            for idx2, data2 in enumerate(data):\n",
    "                if (idx1 > idx2):\n",
    "                    curr_dist_matrix.append(dist_matrix[idx2][idx1])\n",
    "                else:\n",
    "                    dist = self.__get_distance(data1, data2, metrics)\n",
    "                    curr_dist_matrix.append(dist)\n",
    "            dist_matrix.append(curr_dist_matrix)\n",
    "        return dist_matrix\n",
    "        \n",
    "    def fit_predict(self, data):\n",
    "        '''\n",
    "        Fungsi untuk melakukan clustering secara agglomerative\n",
    "        '''\n",
    "        \n",
    "        # preprocessing distance matrix\n",
    "        if (self.linkage != 'average_group'):\n",
    "            dist_matrix = self.__calculate_distance_matrix(data, self.metrics)\n",
    "        # inisiasi cluster dengan satu elemen per cluster awal \n",
    "        clusters = [[i] for i, c in enumerate(data)]\n",
    "\n",
    "        # melakukan iterasi hingga diperoleh jumlah cluster sesuai yang dikehendaki\n",
    "        while(len(clusters) > self.n_clusters):\n",
    "            min_dist = None\n",
    "            merge_pair = (0, 0)\n",
    "            \n",
    "            # mencari cluster dengan jarak terdekat untuk di-merge\n",
    "            for idx1, c1 in enumerate(clusters):\n",
    "                for idx2, c2 in enumerate(clusters[(idx1 + 1) :]):\n",
    "                    if (self.linkage == 'single'):\n",
    "                        dist = self.__single_linkage(c1, c2, dist_matrix)\n",
    "                    elif (self.linkage == 'complete'):\n",
    "                        dist = self.__complete_linkage(c1, c2, dist_matrix)\n",
    "                    elif (self.linkage == 'average'):\n",
    "                        dist = self.__average_linkage(c1, c2, dist_matrix)\n",
    "                    elif (self.linkage == 'average_group'):\n",
    "                        dist = self.__group_average_linkage(c1, c2, data, self.metrics)\n",
    "                    else:\n",
    "                        raise Exception('Linkage not defined')\n",
    "                    if (min_dist == None) or (dist < min_dist):\n",
    "                        min_dist = dist\n",
    "                        merge_pair = (idx1, idx1 + 1 + idx2)\n",
    "            \n",
    "            # merge pasangan cluster dengan jarak terdekat\n",
    "            result_cluster = []\n",
    "            for idx, c in enumerate(clusters):\n",
    "                if idx not in merge_pair:\n",
    "                    result_cluster.append(c)\n",
    "\n",
    "            result_cluster.append(clusters[merge_pair[0]] + clusters[merge_pair[1]])\n",
    "\n",
    "            clusters = result_cluster\n",
    "\n",
    "        # menampilkan hasil clustering\n",
    "        result_per_item = np.full(len(data), 0)\n",
    "        for idx, clust in enumerate(clusters):\n",
    "            result_per_item[clust] = idx\n",
    "\n",
    "        return result_per_item\n",
    "    \n",
    "    '''\n",
    "    Getter untuk parameter\n",
    "    '''\n",
    "    def get_n_cluster(self):\n",
    "        return self.n_clusters\n",
    "    \n",
    "    def get_linkage(self):\n",
    "        return self.linkage\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        return self.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Berikut adalah eksperimen yang dilakukan untuk melakukan *clustering* terhadap *dataset* iris. Digunakan *euclidean distance* dan berbagai variasi perhitungan *linkage* antara dua cluster untuk menerapkan metode *agglomerative clustering*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "label = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering dengan Complete Linkage\n",
    "\n",
    "Pada percobaan ini, digunakan pendekatan *complete linkage* untuk menghitung jarak antar pasangan cluster. Hasil clustering dan *purity*-nya dapat dilihat pada *output* dari eksekusi *code*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 0.8769066333770752 s ----\n",
      "Cluster prediction:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 0 2 0 2 0 2 0 0 0 0 2 0 2 0 0 2 0 2 0 2 2\n",
      " 2 2 2 2 2 0 0 0 0 2 0 2 2 2 0 0 0 2 0 0 0 0 0 2 0 0 2 2 2 2 2 2 0 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Purity: 0.84\n"
     ]
    }
   ],
   "source": [
    "aglo = AgglomerativeClustering(n_clusters=3, linkage='complete', metrics='euclidean')\n",
    "start = time.time()\n",
    "pred = aglo.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering dengan Single Linkage\n",
    "\n",
    "Pada percobaan ini, digunakan pendekatan *single linkage* untuk menghitung jarak antar pasangan cluster. Hasil clustering dan *purity*-nya dapat dilihat pada *output* dari eksekusi *code*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 0.7279071807861328 s ----\n",
      "Cluster prediction:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Purity: 0.68\n"
     ]
    }
   ],
   "source": [
    "aglo = AgglomerativeClustering(n_clusters=3, linkage='single', metrics='euclidean')\n",
    "start = time.time()\n",
    "pred = aglo.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering dengan Average Linkage\n",
    "\n",
    "Pada percobaan ini, digunakan pendekatan *complete linkage* untuk menghitung jarak antar pasangan cluster. Hasil clustering dan *purity*-nya dapat dilihat pada *output* dari eksekusi *code*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 1.2760462760925293 s ----\n",
      "Cluster prediction:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1\n",
      " 1 1 2 2 1 1 1 1 2 1 2 1 2 1 1 2 2 1 1 1 1 1 2 1 1 1 1 2 1 1 1 2 1 1 1 2 1\n",
      " 1 2]\n",
      "Purity: 0.9066666666666666\n"
     ]
    }
   ],
   "source": [
    "aglo = AgglomerativeClustering(n_clusters=3, linkage='average', metrics='euclidean')\n",
    "start = time.time()\n",
    "pred = aglo.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering dengan Average-group Linkage\n",
    "\n",
    "Pada percobaan ini, digunakan pendekatan *average-group linkage* untuk menghitung jarak antar pasangan cluster. Hasil clustering dan *purity*-nya dapat dilihat pada *output* dari eksekusi *code*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 44.81160497665405 s ----\n",
      "Cluster prediction:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0\n",
      " 0 0 2 2 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0\n",
      " 0 2]\n",
      "Purity: 0.9066666666666666\n"
     ]
    }
   ],
   "source": [
    "aglo = AgglomerativeClustering(n_clusters=3, linkage='average_group', metrics='euclidean')\n",
    "start = time.time()\n",
    "pred = aglo.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hasil dan Analisis\n",
    "\n",
    "Dari keempat eksperimen di atas, dapat ditarik kesimpulan bahwa untuk dataset iris, metode *agglomerative* dapat diterapkan untuk melakukan *clustering*. Penggunaan teknik *average linkage* dan *average-group linkage* untuk menghitung jarak antara dua cluster menghasilkan *cluster* dengan *purity* tertinggi, yaitu 0.9067. Namun, *average-group linkage* membutuhkan waktu eksekusi yang lebih lama dibandingkan *average linkage*. Hal ini dikarenakan teknik *average-group linkage* tidak dapat menggunakan hasil *preprocessing distance matrix* sehingga perlu dilakukan komputasi ulang dalam menghitung jarak antara dua *cluster* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "----------------------------\n",
    "\n",
    "DBSCAN merupakan salah satu algoritma clustering yang mengelompokkan data berdasarkan kedekatannya dengan data lain. Data yang dianggap dekat akan dijadikan satu kelompok. Data dianggap dekat dan disebut bertetangga dengan data lainnya apabila jaraknya kurang dari smaa dengan nilai tertentu. Nilai tersebut disebut `epsilon`.\n",
    "\n",
    "Pada DBSCAN satu *instance* data dapat dikategorikan menjadi `core_point`, `border_point`, atau `noise_data`/outlier. Sebuah data disebut `core_point` apabila memiliki jumlah tetangganya lebih dari sama dengan nilai tertentu. Nilai tersebut disebut `min_pts`. Sebuah data dikatakan `border_point` apabila jumlah tetangganya tidak lebih dari `min_pts` namun memiliki tetangga yang merupakan `core_point`. Sedangkan `noise_data` atau outlier adalah data yang jumlah tetangganya tidak lebih dari `min_pts` dan tidak bertetangga dengan `border_point`. \n",
    "\n",
    "Setiap `core_point` dan tetangganya (baik itu `core_point` atau pun `border_point`) akan menjadi satu cluster yang sama. `noise_data` atau outlier merupakan data yang tidak memiliki cluster. \n",
    "\n",
    "Ilustrasi: \n",
    "![DBSCAN](img/dbscan.png \"Ilustrasi DBSCAN\")\n",
    "\n",
    "Pada gambar diatas, titik yang berwarna merah merupakan `core_point`, titik yang berwarna kuning merupakan `border_point` dan titik yang berwarna biru merupakan `noise_data` atau outlier.\n",
    "\n",
    "Perhitungan jarak yang dapat digunakan pada implemetasi DBSCAN ini ada dua macam yaitu jarak euclidean dan jarak manhtattan.\n",
    "\n",
    "Berikut ini merupakan pseudocode dari DBSCAN:\n",
    "\n",
    "```\n",
    "DBSCAN(data, eps, min_pts):\n",
    "    curr_label = 0\n",
    "    for data_i in data:\n",
    "        if data_i is core_point and not yet labelled:\n",
    "            label = curr_label\n",
    "            cluster(data_i) = label\n",
    "            neighbour_stack = [neighbour(data_i)]\n",
    "            while neighbour_stack is not empty:\n",
    "                neighbour_data_i = neighbour_stack.pop\n",
    "                if neighbour_data_i not yet labelled:\n",
    "                    cluster(neighbour_data_i) = label\n",
    "                    if neighbour_data_i is core point:\n",
    "                        neighbour_stack,push(neighbour(neighbour_data_i))\n",
    "           curr_label += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "\n",
    "class tes_DBSCAN:\n",
    "    \n",
    "    UNLABELLED_DATA = -1\n",
    "    \n",
    "    n_clusters = None\n",
    "    result = None\n",
    "        \n",
    "    metrics = 'euclidean'    \n",
    "    eps = 0.5\n",
    "    min_pts = 5\n",
    "    available_metrics = ['euclidean', 'manhattan']\n",
    "\n",
    "    def __init__ (self, min_pts=min_pts, eps=eps, metrics=metrics):\n",
    "        '''\n",
    "        Inisiasi kelas dengan min_pts dan epsilon\n",
    "        '''\n",
    "        if eps <= 0:\n",
    "            raise Exception('eps must be higher than 0')\n",
    "        if min_pts <= 0:\n",
    "            raise Exception('min_pts must be higher than 0')\n",
    "        if metrics not in self.available_metrics:\n",
    "            raise Exception('No metrics \\'' + str(metrics) + '\\'. Available metrics '+ str(self.available_metrics))\n",
    "            \n",
    "        self.min_pts = min_pts\n",
    "        self.eps = eps\n",
    "        self.metrics=metrics\n",
    "        \n",
    "    def __euclidean_distance(self, point_a, point_b):\n",
    "        '''\n",
    "        Fungsi untuk menghitung euclidean distance\n",
    "        '''\n",
    "        dist = 0\n",
    "        for a, b in zip(point_a, point_b):\n",
    "            dist += (a - b) * (a - b)\n",
    "        return np.sqrt(dist)\n",
    "\n",
    "    def __manhattan_distance(self, point_a, point_b):\n",
    "        '''\n",
    "        Fungsi untuk menghitung manhattan distance \n",
    "        '''\n",
    "        dist = 0\n",
    "        for a, b in zip(point_a, point_b):\n",
    "            dist += abs(a - b)\n",
    "        return dist\n",
    "    \n",
    "    def __distance(self, point_a, point_b, metrics=metrics):\n",
    "        '''\n",
    "        Fungsi untuk mencari jarak berdasarkan metricsnya\n",
    "        '''\n",
    "        if len(point_a) == len(point_b):\n",
    "            if metrics == 'euclidean':\n",
    "                return self.__euclidean_distance(point_a, point_b)\n",
    "            if metrics == 'manhattan':\n",
    "                return self.__manhattan_distance(point_a, point_b)\n",
    "        else:\n",
    "            raise Exception(\"feature length doesn't same\")\n",
    "    \n",
    "    def fit_predict(self, data):\n",
    "        '''\n",
    "        Fungsi untuk mengelompolkkan data\n",
    "        '''\n",
    "        size_data = len(data)\n",
    "        \n",
    "        # generate all neighbours \n",
    "        neighbours = []\n",
    "        for i in range(size_data):\n",
    "            neighbour_i = []\n",
    "            for j in range(size_data):\n",
    "                if self.__distance(data[i], data[j], self.metrics) <= self.eps:\n",
    "                    neighbour_i.append(j)\n",
    "            neighbours.append(neighbour_i)\n",
    "        \n",
    "        # initialize label\n",
    "        self.result = np.full((size_data), self.UNLABELLED_DATA)\n",
    "        \n",
    "        # giving label to data\n",
    "        curr_label = 0\n",
    "        for i in range(size_data):\n",
    "            # if neighbours > min_pts (data_i is core points) and not yet labelled, then give label \n",
    "            if len(neighbours[i]) >= self.min_pts and self.result[i] == self.UNLABELLED_DATA: \n",
    "                label = curr_label\n",
    "                # giving label to all neighbours\n",
    "                neighbours_i = [i]\n",
    "                while len(neighbours_i) > 0:\n",
    "                    neigh_i = neighbours_i.pop()\n",
    "                    # if not yet labelled then give label to data and the neighbours\n",
    "                    if self.result[neigh_i] == self.UNLABELLED_DATA:\n",
    "                        self.result[neigh_i] = label\n",
    "                        # if neigh_i is core point, then give label to the neighbour\n",
    "                        if len(neighbours[neigh_i]) >= self.min_pts:\n",
    "                            neighbours_i += neighbours[neigh_i]\n",
    "                curr_label += 1\n",
    "        \n",
    "        self.n_clusters = curr_label           \n",
    "        return self.result\n",
    "    \n",
    "    def get_n_clusters(self):\n",
    "        if self.n_clusters is None:\n",
    "            print(\"No data\")\n",
    "        else:\n",
    "            return self.n_clusters\n",
    "    \n",
    "    def get_epsilon(self):\n",
    "        return self.eps\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return self.metrics\n",
    "    \n",
    "    def get_min_pts(self):\n",
    "        return self.min_pts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Berikut ini merupakan hasil eksperimen implementasi DBSCAN untuk clustering data iris menggunakan *euclidean disntance*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "label = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 0.3169534206390381 s ----\n",
      "---- Cluster: 3 ----\n",
      "Cluster prediction:\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0\n",
      "  0  0  1  1  1  1  1  1  1  2  1  1  2  1  1  1  1  1  1  1 -1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  2  1  1\n",
      "  1  1  2  1  1  1  1  1  1 -1 -1  1 -1 -1  1  1  1  1  1  1  1 -1 -1  1\n",
      "  1  1 -1  1  1  1  1  1  1  1  1 -1  1  1 -1 -1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1]\n",
      "Purity: 0.708029197080292\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "dbscan = tes_DBSCAN(eps=0.5, min_pts=4, metrics='euclidean')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pred = dbscan.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"---- Cluster: {} ----\".format(dbscan.get_n_clusters()))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 0.2816331386566162 s ----\n",
      "---- Cluster: 2 ----\n",
      "Cluster prediction:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1]\n",
      "Purity: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "dbscan = tes_DBSCAN(eps=1, min_pts=3, metrics='euclidean')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "pred = dbscan.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"---- Cluster: {} ----\".format(dbscan.get_n_clusters()))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hasil Eksperimen\n",
    "\n",
    "Dari kedua hasil eksperimen diatas, dapat dilihat bahwa DBSCAN mampu mengelompokkan data iris dalam waktu sekitar 0.2 - 0.3  detik. Namun kedua eksperimen menghasilkan nilai purity yang berbeda, hal ini dikarenakan perbedaan nilai `epsilon` dan `min_pts`. Dengan nilai `epsilon` 0.5 dan `min_pts` 4, DBSCAN menghasilkan 3 kluster dan beberapa data yang dianggap outlier. Sedangkan dengan nilai `epsilon` 1 dan `min_pts` 3, DBSCAN menghasilkan 2 kluster dan tanpa ada data yang dianggap outlier. Hal ini menunjukkan bahwa DBSCAN akan sangat bergantung terhadap kedua nilai tersebut dan ini merupakan tantangan dalam menggunakan DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans\n",
    "------------------------------------------\n",
    "*KMeans* merupakan suatu teknik dalam melakukan *clustering* yang menggunakan pendekatan *partitioning*. Ide utama dari teknik ini adalah dengan mengelompokkan data ke K *cluster*. Penentuan *cluster* mana untuk data tertentu yaitu dengan cara menghitung jarak data tersebut dengan representasi dari *cluster* (*centroid*). Representasi dari *cluster* berupa rata-rata (*means*) dari data-data anggota *cluster* tersebut. Penghitungan jarak dengan menggunakan *euclidean distance*.\n",
    "\n",
    "![kmeans](img/kmeans.png \"KMeans Clustering\")\n",
    "\n",
    "Dalam notasi *pseudocode*, algoritma *KMeans* adalah sebagai berikut.\n",
    "\n",
    "```\n",
    "choose_initial_centroid()\n",
    "WHILE (any_change_in_cluster) DO\n",
    "    FOREACH object\n",
    "        assign_to_cluster(object)\n",
    "    FOREACH cluster\n",
    "        update_cluster_means(cluster)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "class KMeans:\n",
    "    '''\n",
    "    Kelas untuk mengakomodasi nilai metode KMeans clustering\n",
    "    '''\n",
    "    # Nilai default parameter\n",
    "    n_clusters = 2\n",
    "    init = 'random'\n",
    "    max_iter = 300\n",
    "    init_val = []\n",
    "    \n",
    "    available_init = ['random', 'manual']\n",
    "    \n",
    "    def __init__(self, n_clusters=n_clusters, init=init, init_val=init_val, max_iter=max_iter):\n",
    "        '''\n",
    "        Inisiasi kelas. Parameter yang dibutuhkan untuk setiap kelas diinisiasi atau diisi dengan nilai default \n",
    "        '''    \n",
    "        if n_clusters <= 0:\n",
    "            raise Exception('n_clusters must be higher than 0')\n",
    "        if init not in self.available_init:\n",
    "            raise Exception('No init method \\'' + str(init) + '\\'. Available init methods'+ str(self.available_init))\n",
    "        if (init == 'manual' and len(init_val) != n_clusters):\n",
    "            raise Exception('init_val length doesn\\'t match with n_clusters '+ str(n_clusters))\n",
    "        self.n_clusters = n_clusters\n",
    "        self.init = init\n",
    "        self.max_iter = max_iter\n",
    "        self.init_val = init_val\n",
    "    \n",
    "    def __is_in_array(self, data, arr):\n",
    "        '''\n",
    "        Fungsi helper untuk menegecek apakah data berupa array berada pada arr berupa array of array\n",
    "        '''\n",
    "        is_exist = False\n",
    "        arr_idx = 0\n",
    "        while (not is_exist and arr_idx < len(arr)):\n",
    "            is_data_equal = True\n",
    "            data_idx = 0\n",
    "            while (is_data_equal and data_idx < len(data)):\n",
    "                if (data[data_idx] != arr[arr_idx][data_idx]):\n",
    "                    is_data_equal = False\n",
    "                else:\n",
    "                    data_idx += 1\n",
    "            if is_data_equal:\n",
    "                is_exist = True\n",
    "            else:\n",
    "                arr_idx += 1\n",
    "        return is_exist\n",
    "        \n",
    "    def __euclidean_distance(self, data1, data2):\n",
    "        '''\n",
    "        Fungsi untuk menghitung euclidean distance di antara dua vector dengan panjang yang sama\n",
    "        '''\n",
    "        sum = 0\n",
    "        if (len(data1) == len(data2)):\n",
    "            for x1, x2 in zip(data1, data2):\n",
    "                sum += (x1 - x2)**2\n",
    "            dist = math.sqrt(sum)\n",
    "            return dist\n",
    "        else:\n",
    "            raise Exception('Length doesn\\'t match')\n",
    "            \n",
    "    def __get_distance(self, data1, data2):\n",
    "        '''\n",
    "        Fungsi untuk menghitung jarak dua vector\n",
    "        '''\n",
    "        return self.__euclidean_distance(data1, data2)\n",
    "        \n",
    "    def __calculate_distance_matrix(self, data, centroids):\n",
    "        '''\n",
    "        Fungsi untuk menghitung distance matrix untuk semua data dengan centroid\n",
    "        '''\n",
    "        dist_matrix = []        \n",
    "        for i in range(len(centroids)):\n",
    "            dist_curr_centroid = []\n",
    "            for j in range(len(data)):\n",
    "                dist = self.__get_distance(centroids[i], data[j])\n",
    "                dist_curr_centroid.append(dist)\n",
    "            dist_matrix.append(dist_curr_centroid)\n",
    "        \n",
    "        return dist_matrix\n",
    "    \n",
    "    def __assign_data_to_cluster(self, dist_matrix):\n",
    "        '''\n",
    "        Fungsi untuk mengelompokan data berdasarkan jarak yang diketahui\n",
    "        '''\n",
    "        cluster_of_data = []\n",
    "        for j in range(len(dist_matrix[0])):\n",
    "            cluster = 0\n",
    "            min_distance = dist_matrix[0][j]\n",
    "            for i in range(1,len(dist_matrix)):\n",
    "                if (dist_matrix[i][j] < min_distance):\n",
    "                    cluster = i\n",
    "                    min_distance = dist_matrix[i][j]\n",
    "            cluster_of_data.append(cluster)\n",
    "        return cluster_of_data\n",
    "        \n",
    "    def __get_centroids(self, data, cluster_of_data):\n",
    "        '''\n",
    "        Fungsi untuk menghitung centroid baru\n",
    "        '''\n",
    "        centroids = []\n",
    "        data_per_cluster = []\n",
    "        # inisiasi\n",
    "        for n in range(self.n_clusters):\n",
    "            data_per_cluster.append([])\n",
    "        # masukkan data ke array tiap cluster\n",
    "        for idx, data_cluster in enumerate(cluster_of_data):\n",
    "            data_per_cluster[data_cluster].append(data[idx])\n",
    "        # hitung means\n",
    "        for n in range(self.n_clusters):\n",
    "            if len(data_per_cluster[n]) > 0:\n",
    "                means_cluster = []\n",
    "                for column in range(len(data_per_cluster[n][0])):\n",
    "                    sum_column = 0\n",
    "                    for d in data_per_cluster[n]:\n",
    "                        sum_column += d[column]\n",
    "                    means_column = sum_column / len(data_per_cluster[n])\n",
    "                    means_cluster.append(means_column)\n",
    "                centroids.append(means_cluster)\n",
    "        return centroids\n",
    "        \n",
    "    def fit_predict(self, data):\n",
    "        '''\n",
    "        Fungsi untuk melakukan clustering secara KMeans\n",
    "        '''\n",
    "        \n",
    "        cluster_of_data = []\n",
    "        \n",
    "        # initiate centroid\n",
    "        centroids = []\n",
    "        if (self.init == 'random'):\n",
    "            # cek keunikan data\n",
    "            unique_data_idx = []\n",
    "            unique_data = []\n",
    "            i = 0\n",
    "            while (len(unique_data_idx) < self.n_clusters) and (i < len(data)):\n",
    "                if not self.__is_in_array(data[i], unique_data):\n",
    "                    unique_data_idx.append(i)\n",
    "                    unique_data.append(data[i])\n",
    "                i += 1\n",
    "                \n",
    "            if (len(unique_data_idx) < self.n_clusters):\n",
    "                # jika keunikan data kurang dari n_clusters\n",
    "                for u in unique_data_idx:\n",
    "                    curr_centroid = np.copy(data[u])\n",
    "                    centroids.append(curr_centroid)\n",
    "                for i in range(self.n_clusters - len(unique_data_idx)):\n",
    "                    rand_idx = random.randint(-1,len(data)-1)\n",
    "                    # cek apakah sudah terpilih atau belum\n",
    "                    while (rand_idx in unique_data_idx):\n",
    "                        rand_idx = random.randint(-1,len(data)-1)\n",
    "                    curr_centroid = np.copy(data[rand_idx])\n",
    "                    centroids.append(curr_centroid)\n",
    "            else:\n",
    "                for i in range(self.n_clusters):\n",
    "                    rand_idx = random.randint(-1,len(data)-1)\n",
    "                    curr_centroid = np.copy(data[rand_idx])\n",
    "                    # cek apakah sudah terpilih atau belum\n",
    "                    while (self.__is_in_array(curr_centroid, centroids)):\n",
    "                        rand_idx = random.randint(-1,len(data)-1)\n",
    "                        curr_centroid = np.copy(data[rand_idx])\n",
    "                    centroids.append(curr_centroid)\n",
    "        else:\n",
    "            # self.init == 'manual'\n",
    "            centroids = self.init_val\n",
    "        \n",
    "        iteration = 1\n",
    "        is_convergen = False\n",
    "        while (not is_convergen and iteration <= self.max_iter):\n",
    "            # calculate distance all data to all centroid\n",
    "            dist_matrix = self.__calculate_distance_matrix(data, centroids)\n",
    "            # assign all data to cluster\n",
    "            new_cluster_of_data = self.__assign_data_to_cluster(dist_matrix)\n",
    "            # convergency checking\n",
    "            is_convergen = np.array_equal(cluster_of_data, new_cluster_of_data)\n",
    "            # for next iteration\n",
    "            if not is_convergen:\n",
    "                cluster_of_data = np.copy(new_cluster_of_data)\n",
    "                centroids = self.__get_centroids(data, cluster_of_data)\n",
    "                iteration += 1\n",
    "    \n",
    "        return cluster_of_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Berikut ini merupakan hasil eksperimen implementasi KMeans untuk clustering data iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering dengan k=3\n",
    "\n",
    "Pada percobaan ini, dipilih nilai k=3 dan dilakukan percobaan sebanyak 2 kali. Hasil clustering dan *purity*-nya dapat dilihat pada *output* dari eksekusi *code*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 0.0557103157043457 s ----\n",
      "Cluster prediction:\n",
      "[2 1 1 1 2 2 1 2 1 1 2 1 1 1 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 1 1 2 2 2 1 2 2\n",
      " 1 1 2 2 1 1 2 2 1 2 1 2 2 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "Purity: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "start = time.time()\n",
    "pred = kmeans.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 0.04532933235168457 s ----\n",
      "Cluster prediction:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2\n",
      " 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2\n",
      " 2 1]\n",
      "Purity: 0.8866666666666667\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "start = time.time()\n",
    "pred = kmeans.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering dengan k=4\n",
    "\n",
    "Pada percobaan ini, dipilih nilai k=4 dan dilakukan percobaan sebanyak 2 kali. Hasil clustering dan *purity*-nya dapat dilihat pada *output* dari eksekusi *code*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 0.07248306274414062 s ----\n",
      "Cluster prediction:\n",
      "[3 1 1 1 3 3 1 3 1 1 3 1 1 1 3 3 3 3 3 3 3 3 1 3 1 1 3 3 3 1 1 3 3 3 1 1 3\n",
      " 1 1 3 3 1 1 3 3 1 3 1 3 1 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n",
      " 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2\n",
      " 2 0]\n",
      "Purity: 0.8866666666666667\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "start = time.time()\n",
    "pred = kmeans.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 0.030618667602539062 s ----\n",
      "Cluster prediction:\n",
      "[0 2 2 2 0 0 2 2 2 2 0 2 2 2 0 0 0 0 0 0 0 0 2 0 2 2 0 0 0 2 2 0 0 0 2 2 0\n",
      " 2 2 0 0 2 2 0 0 2 0 2 0 2 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 3 3 3 3 1 3 3 3 3\n",
      " 3 3 1 1 3 3 3 3 1 3 1 3 1 3 3 1 1 3 3 3 3 3 1 3 3 3 3 1 3 3 3 1 3 3 3 1 3\n",
      " 3 1]\n",
      "Purity: 0.8933333333333333\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=4)\n",
    "start = time.time()\n",
    "pred = kmeans.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hasil dan Analisis\n",
    "\n",
    "Dari keempat eksperimen di atas, dapat ditarik kesimpulan bahwa untuk dataset iris, metode *KMeans* dapat diterapkan untuk melakukan *clustering*. Pemilihan nilai k=4 menghasilkan *cluster* dengan *purity* tertinggi, yaitu 0.893. Baik nilai k=3 atau k=4 membutuhkan waktu eksekusi yang relatif sama 0.01 detik. Terjadi perbedaan antara percobaan pertama dan kedua pada k=3, hal ini dikarenakan inisiasi centroid yang random membuat hasil yang tidak sama. Meskipun pada k=4, percobaan pertama dan kedua menghasilkan *purity* yang sama, namun untuk percobaan ketiga dan selanjutnya belum tentu hasil yang sama akan didapatkan karena penggunaan teknik *kmeans* ini sangat sensitif terhadap inisiasi centroidnya. Untuk itu, sebaiknya dilakukan percobaan berulang-ulang hingga mendapatkan hasil yang terbaik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMedoid (PAM)\n",
    "------------------------------------------\n",
    "*KMedoid* merupakan suatu teknik dalam melakukan *clustering* yang menggunakan pendekatan *partitioning*. Ide utama dari teknik ini adalah dengan mengelompokkan data ke K *cluster*. Penentuan *cluster* mana untuk data tertentu yaitu dengan cara menghitung jarak data tersebut dengan representasi dari *cluster* (*centroid*). Representasi dari *cluster* berupa sebuah data (*medoid*) dari data-data anggota *cluster* tersebut. Penghitungan jarak dengan menggunakan *manhattan distance*. *Medoid* akan di-update setiap iterasi dengan cara pemilihan secara random pada cluster tertentu, kemudian dihitung perubahan *error*. *Error* berupa *absolute error*, jika perubahan *error* < 0, maka iterasi akan berlanjut dan *medoid* di-update. \n",
    "\n",
    "![kmedoid](img/kmedoid.png \"KMedoid Clustering\")\n",
    "\n",
    "Dalam notasi *pseudocode*, algoritma *KMedoid* adalah sebagai berikut.\n",
    "\n",
    "```\n",
    "choose_initial_centroid()\n",
    "REPEAT\n",
    "    FOREACH object\n",
    "        assign_to_cluster(object)\n",
    "    choose_new_centroid()\n",
    "    calculate_total_error()\n",
    "    if (total_error < 0)\n",
    "        swap(old_centroid, new_centroid)\n",
    "UNTIL no_changes\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "class KMedoid:\n",
    "    '''\n",
    "    Kelas untuk mengakomodasi nilai metode KMedoid clustering\n",
    "    '''\n",
    "    # Nilai default parameter\n",
    "    n_clusters = 2\n",
    "    init = 'random'\n",
    "    max_iter = 300\n",
    "    init_val = []\n",
    "    randomize_cluster = 0\n",
    "    choosen_cluster = []\n",
    "    \n",
    "    available_init = ['random', 'manual']\n",
    "    \n",
    "    def __init__(self, n_clusters=n_clusters, init=init, init_val=init_val,\n",
    "                 max_iter=max_iter, randomize_cluster=randomize_cluster):\n",
    "        '''\n",
    "        Inisiasi kelas. Parameter yang dibutuhkan untuk setiap kelas diinisiasi atau diisi dengan nilai default \n",
    "        '''\n",
    "        if n_clusters <= 0:\n",
    "            raise Exception('n_clusters must be higher than 0')\n",
    "        if init not in self.available_init:\n",
    "            raise Exception('No init method \\'' + str(init) + '\\'. Available init methods'+ str(self.available_init))\n",
    "        if (init == 'manual' and len(init_val) != n_clusters):\n",
    "            raise Exception('init_val length doesn\\'t match with n_clusters '+ str(n_clusters))\n",
    "        if (n_clusters-1 < randomize_cluster) or (randomize_cluster < 0):\n",
    "            raise Exception('randomize_cluster must be between 0 and n_clusters-1')\n",
    "        self.n_clusters = n_clusters\n",
    "        self.init = init\n",
    "        self.max_iter = max_iter\n",
    "        self.init_val = init_val\n",
    "        self.randomize_cluster = randomize_cluster\n",
    "    \n",
    "    def __is_in_array(self, data, arr):\n",
    "        '''\n",
    "        Fungsi helper untuk menegecek apakah data berupa array berada pada arr berupa array of array\n",
    "        '''\n",
    "        is_exist = False\n",
    "        arr_idx = 0\n",
    "        while (not is_exist and arr_idx < len(arr)):\n",
    "            is_data_equal = True\n",
    "            data_idx = 0\n",
    "            while (is_data_equal and data_idx < len(data)):\n",
    "                if (data[data_idx] != arr[arr_idx][data_idx]):\n",
    "                    is_data_equal = False\n",
    "                else:\n",
    "                    data_idx += 1\n",
    "            if is_data_equal:\n",
    "                is_exist = True\n",
    "            else:\n",
    "                arr_idx += 1\n",
    "        return is_exist\n",
    "        \n",
    "    def __manhattan_distance(self, data1, data2):\n",
    "        '''\n",
    "        Fungsi untuk menghitung manhattan distance di antara dua vector dengan panjang yang sama\n",
    "        '''\n",
    "        sum = 0\n",
    "        if (len(data1) == len(data2)):\n",
    "            for x1, x2 in zip(data1, data2):\n",
    "                sum += abs(x1 - x2)\n",
    "            return sum\n",
    "        else:\n",
    "            raise Exception('Length doesn\\'t match')\n",
    "            \n",
    "    def __get_distance(self, data1, data2):\n",
    "        '''\n",
    "        Fungsi untuk menghitung jarak dua vector\n",
    "        '''\n",
    "        return self.__manhattan_distance(data1, data2)\n",
    "        \n",
    "    def __calculate_distance_matrix(self, data, centroids):\n",
    "        '''\n",
    "        Fungsi untuk menghitung distance matrix untuk semua data dengan centroid\n",
    "        '''\n",
    "        dist_matrix = []        \n",
    "        for i in range(len(centroids)):\n",
    "            dist_curr_centroid = []\n",
    "            for j in range(len(data)):\n",
    "                dist = self.__get_distance(centroids[i], data[j])\n",
    "                dist_curr_centroid.append(dist)\n",
    "            dist_matrix.append(dist_curr_centroid)\n",
    "        \n",
    "        return dist_matrix\n",
    "    \n",
    "    def __assign_data_to_cluster(self, dist_matrix):\n",
    "        '''\n",
    "        Fungsi untuk mengelompokan data berdasarkan jarak yang diketahui\n",
    "        '''\n",
    "        cluster_of_data = []\n",
    "        for j in range(len(dist_matrix[0])):\n",
    "            cluster = 0\n",
    "            min_distance = dist_matrix[0][j]\n",
    "            for i in range(1,len(dist_matrix)):\n",
    "                if (dist_matrix[i][j] < min_distance):\n",
    "                    cluster = i\n",
    "                    min_distance = dist_matrix[i][j]\n",
    "            cluster_of_data.append(cluster)\n",
    "        return cluster_of_data\n",
    "    \n",
    "    def __get_centroids(self, data, cluster_of_data, centroids):\n",
    "        '''\n",
    "        Fungsi untuk mendapatkan centroid baru\n",
    "        '''\n",
    "        # centroid candidate\n",
    "        data_of_randomize_cluster = []\n",
    "        for idx, data_cluster in enumerate(cluster_of_data):\n",
    "            if data_cluster == self.randomize_cluster:\n",
    "                data_of_randomize_cluster.append(data[idx])\n",
    "        \n",
    "        # choose random\n",
    "        idx = 0\n",
    "        stop = False\n",
    "        while ( not stop and idx < len(data_of_randomize_cluster)\n",
    "        ):\n",
    "            new_centroid = np.copy(data_of_randomize_cluster[idx])\n",
    "            if (\n",
    "                self.__get_distance(new_centroid, centroids[self.randomize_cluster]) == 0 or \n",
    "                self.__is_in_array(new_centroid, self.choosen_cluster)\n",
    "            ):\n",
    "                idx += 1\n",
    "            else:\n",
    "                stop = True\n",
    "        \n",
    "        if not self.__is_in_array(new_centroid, self.choosen_cluster):\n",
    "            self.choosen_cluster.append(new_centroid)\n",
    "        \n",
    "        new_centroids = np.copy(centroids)\n",
    "        new_centroids[self.randomize_cluster] = new_centroid\n",
    "        return new_centroids\n",
    "    \n",
    "    def __calculate_error(self, data, cluster_of_data, new_cluster_of_data, centroids, new_centroids):\n",
    "        '''\n",
    "        Fungsi untuk menghitung total absolute error\n",
    "        '''\n",
    "        \n",
    "        old_error = 0\n",
    "        new_error = 0\n",
    "        for n in range(self.n_clusters):\n",
    "            for idx, val in enumerate(data):\n",
    "                old_error += self.__get_distance(val, centroids[cluster_of_data[idx]])\n",
    "                new_error += self.__get_distance(val, new_centroids[new_cluster_of_data[idx]])\n",
    "        \n",
    "        return new_error-old_error\n",
    "        \n",
    "    def fit_predict(self, data):\n",
    "        '''\n",
    "        Fungsi untuk melakukan clustering secara KMedoid\n",
    "        '''\n",
    "        \n",
    "        cluster_of_data = []\n",
    "        \n",
    "        # initiate centroid\n",
    "        centroids = []\n",
    "        if (self.init == 'random'):\n",
    "            # cek keunikan data\n",
    "            unique_data_idx = []\n",
    "            unique_data = []\n",
    "            i = 0\n",
    "            while (len(unique_data_idx) < self.n_clusters) and (i < len(data)):\n",
    "                if not self.__is_in_array(data[i], unique_data):\n",
    "                    unique_data_idx.append(i)\n",
    "                    unique_data.append(data[i])\n",
    "                i += 1\n",
    "                \n",
    "            if (len(unique_data_idx) < self.n_clusters):\n",
    "                # jika keunikan data kurang dari n_clusters\n",
    "                for u in unique_data_idx:\n",
    "                    curr_centroid = np.copy(data[u])\n",
    "                    centroids.append(curr_centroid)\n",
    "                for i in range(self.n_clusters - len(unique_data_idx)):\n",
    "                    rand_idx = random.randint(-1,len(data)-1)\n",
    "                    # cek apakah sudah terpilih atau belum\n",
    "                    while (rand_idx in unique_data_idx):\n",
    "                        rand_idx = random.randint(-1,len(data)-1)\n",
    "                    curr_centroid = np.copy(data[rand_idx])\n",
    "                    centroids.append(curr_centroid)\n",
    "            else:\n",
    "                for i in range(self.n_clusters):\n",
    "                    rand_idx = random.randint(-1,len(data)-1)\n",
    "                    curr_centroid = np.copy(data[rand_idx])\n",
    "                    # cek apakah sudah terpilih atau belum\n",
    "                    while (self.__is_in_array(curr_centroid, centroids)):\n",
    "                        rand_idx = random.randint(-1,len(data)-1)\n",
    "                        curr_centroid = np.copy(data[rand_idx])\n",
    "                    centroids.append(curr_centroid)\n",
    "        else:\n",
    "            # self.init == 'manual'\n",
    "            centroids = self.init_val\n",
    "        \n",
    "        iteration = 1\n",
    "        is_convergen = False\n",
    "        \n",
    "        while (not is_convergen and iteration <= self.max_iter):\n",
    "            # calculate distance all data to all centroid\n",
    "            dist_matrix = self.__calculate_distance_matrix(data, centroids)\n",
    "            # assign all data to cluster\n",
    "            cluster_of_data = self.__assign_data_to_cluster(dist_matrix)\n",
    "            # get new possible centroid\n",
    "            new_centroids = self.__get_centroids(data, cluster_of_data, centroids)\n",
    "            # calculate distance all data to all new centroid\n",
    "            new_dist_matrix = self.__calculate_distance_matrix(data, new_centroids)\n",
    "            # assign all data to new cluster\n",
    "            new_cluster_of_data = self.__assign_data_to_cluster(new_dist_matrix)\n",
    "            # convergency checking\n",
    "            if (self.__calculate_error(data, cluster_of_data, new_cluster_of_data, centroids, new_centroids) >= 0):\n",
    "                is_convergen = True\n",
    "            \n",
    "            # for next iteration\n",
    "            if not is_convergen:\n",
    "                cluster_of_data = np.copy(new_cluster_of_data)\n",
    "                centroids = np.copy(new_centroids)\n",
    "                iteration += 1\n",
    "                \n",
    "        return np.array(cluster_of_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Berikut ini merupakan hasil eksperimen implementasi KMedoid (PAM) untuk clustering data iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering dengan k=4, randomize_cluster=0\n",
    "\n",
    "Pada percobaan ini, dipilih nilai k=4 dan randomize_cluster=0, artinya cluster yang akan dirandom centroidnya untuk setiap iterasi adalah cluster pertama. Hasil clustering dan *purity*-nya dapat dilihat pada *output* dari eksekusi *code*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 0.09967494010925293 s ----\n",
      "Cluster prediction:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 0 0 0 3 0 3 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0\n",
      " 0 3 3 3 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n",
      " 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 0 2 2 2 0 2 2 2 0 2 2 2 0 2\n",
      " 2 0]\n",
      "Purity: 0.9\n"
     ]
    }
   ],
   "source": [
    "kmedoid = KMedoid(n_clusters=4, randomize_cluster=0)\n",
    "start = time.time()\n",
    "pred = kmedoid.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering dengan k=4, randomize_cluster=1\n",
    "\n",
    "Pada percobaan ini, dipilih nilai k=4 dan randomize_cluster=1, artinya cluster yang akan dirandom centroidnya untuk setiap iterasi adalah cluster kedua. Hasil clustering dan *purity*-nya dapat dilihat pada *output* dari eksekusi *code*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 0.0945584774017334 s ----\n",
      "Cluster prediction:\n",
      "[3 0 0 0 3 1 3 3 0 0 1 3 0 0 1 1 1 3 1 3 1 3 3 3 3 0 3 3 3 0 0 1 1 1 0 3 1\n",
      " 0 0 3 3 0 0 3 3 0 3 0 1 3 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "Purity: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "kmedoid = KMedoid(n_clusters=4, randomize_cluster=1)\n",
    "start = time.time()\n",
    "pred = kmedoid.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering dengan k=4, randomize_cluster=2\n",
    "\n",
    "Pada percobaan ini, dipilih nilai k=4 dan randomize_cluster=2, artinya cluster yang akan dirandom centroidnya untuk setiap iterasi adalah cluster ketiga. Hasil clustering dan *purity*-nya dapat dilihat pada *output* dari eksekusi *code*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 0.06009364128112793 s ----\n",
      "Cluster prediction:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 3 2 3 2 3 2 3 3 3 3 2 3 2 2 3 2 3 2 3 2 2\n",
      " 2 2 2 2 2 3 3 3 3 1 2 2 2 2 3 3 3 2 3 3 3 3 3 2 3 3 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 2 1 2 1 1 1 2 2 1 1 1 2 1 1 1 1 1 1 1 1 1\n",
      " 1 1]\n",
      "Purity: 0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "kmedoid = KMedoid(n_clusters=4, randomize_cluster=2)\n",
    "start = time.time()\n",
    "pred = kmedoid.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering dengan k=4, randomize_cluster=3\n",
    "\n",
    "Pada percobaan ini, dipilih nilai k=4 dan randomize_cluster=0, artinya cluster yang akan dirandom centroidnya untuk setiap iterasi adalah cluster keempat. Hasil clustering dan *purity*-nya dapat dilihat pada *output* dari eksekusi *code*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Time taken: 0.05040931701660156 s ----\n",
      "Cluster prediction:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 2 0 2 3 2 3 2 2 2 2 0 2 3 2 2 0 2 3 2 0 2\n",
      " 0 3 3 3 0 2 2 2 2 0 2 0 3 0 2 2 2 0 2 2 2 2 2 0 2 2 3 2 3 3 3 3 0 3 3 3 3\n",
      " 3 3 0 2 3 3 3 3 0 3 0 3 0 3 3 0 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 2 3 3 3 0 3\n",
      " 3 3]\n",
      "Purity: 0.8533333333333334\n"
     ]
    }
   ],
   "source": [
    "kmedoid = KMedoid(n_clusters=4, randomize_cluster=3)\n",
    "start = time.time()\n",
    "pred = kmedoid.fit_predict(data)\n",
    "\n",
    "print(\"---- Time taken: {} s ----\".format(time.time() - start))\n",
    "print(\"Cluster prediction:\")\n",
    "print(pred)\n",
    "print(\"Purity: {}\".format(purity(pred, label)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hasil dan Analisis\n",
    "\n",
    "Dari keempat eksperimen di atas, dapat ditarik kesimpulan bahwa untuk dataset iris, metode *KMeans (PAM)* dapat diterapkan untuk melakukan *clustering*. Pemilihan nilai randomize_cluster=2 menghasilkan *cluster* dengan *purity* tertinggi, yaitu 0.9467. Keempat percobaan diatas menghasilkan *purity* yang berbeda meskipun nilai k sama, yaitu k=4. Hal ini dikarenakan inisiasi centroid yang random membuat hasil yang tidak sama serta pemilihan centroid mana yang akan di-random untuk setiap iterasinnya juga mempengaruhi hasil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pembagian Kerja\n",
    "-------------------------\n",
    "Berikut adalah pembagian kerja yang dilakukan selama pelaksanaan tugas besar ini.\n",
    "\n",
    "|      Nama     |                           Deskripsi Pekerjaan                          |\n",
    "|:-------------:|:----------------------------------------------------------------------:|\n",
    "| Diki Ardian   | Implementasi, eksperimen, dan analisis metode Kmeans dan Kmedoids      |\n",
    "| Irfan Ariq    | Implementasi, eksperimen, dan analisis metode DBSCAN                   |\n",
    "| Pratamamia AP | Implementasi, eksperimen, dan analisis metode Agglomerative Clustering |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
