{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_epochs = 100,\n",
    "        momentum = 0.5,\n",
    "        learning_rate = 0.0001,\n",
    "        batch_size = 1,\n",
    "        neuron_hidden_layer = [3,3,3],\n",
    "        f_activation = 'sigmoid'\n",
    "    ):\n",
    "    \n",
    "        if (batch_size <= 0):\n",
    "            raise Exception(\"batch size must be positive\")\n",
    "        if (learning_rate <= 0):\n",
    "            raise Exception(\"learning rate must be positive\")\n",
    "        if (momentum < 0):\n",
    "            raise Exception(\"momentum cannot be negative\")\n",
    "        if len(neuron_hidden_layer) == 0 or len(neuron_hidden_layer) == 10:\n",
    "            raise Exception(\"learning_rate must be > 0 and <= 10\")\n",
    "    \n",
    "        self.max_epochs = max_epochs\n",
    "        self.momentum = momentum\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.neuron_hidden_layer = neuron_hidden_layer\n",
    "        self.f_activation = f_activation\n",
    "    \n",
    "    def sigmoid(self, value):\n",
    "        return 1 / (1 + np.exp(-value))\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def __activation(self, value , func = 'sigmoid'):\n",
    "        if func == 'sigmoid':\n",
    "            return self.sigmoid(value)\n",
    "        \n",
    "    def __activation_derivative(self, value , func = 'sigmoid'):\n",
    "        if func == 'sigmoid':\n",
    "            return self.__sigmoid_derivative(value)\n",
    "        \n",
    "    def __error_derivative(self, pred, actual):\n",
    "        return (pred - actual) / (pred - np.square(pred))\n",
    "        \n",
    "    def __error(self, pred, actual):\n",
    "        return -(np.log(pred) if actual > 0 else np.log(1.0 - pred))\n",
    "\n",
    "    def __init_neuron_weight(self, n_feature):\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "        prev_input = n_feature\n",
    "        for i, n_neuron in enumerate(self.neuron_hidden_layer):\n",
    "            limit = np.sqrt(6 / (prev_input + n_neuron))\n",
    "            self.weights.append(np.random.rand(n_neuron, prev_input))\n",
    "            self.bias.append(np.random.rand(n_neuron))\n",
    "            prev_input = n_neuron\n",
    "        limit = np.sqrt(6 / (prev_input + n_neuron))\n",
    "        self.weights.append(np.random.rand(1, prev_input))\n",
    "        self.weights = np.array(self.weights)\n",
    "        self.bias.append(np.random.rand(1))\n",
    "        self.bias = np.array(self.bias)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights, self.bias\n",
    "    \n",
    "    def predict(self, data):\n",
    "        result = []\n",
    "        for x in data:\n",
    "            prev_input = x\n",
    "            for i, (w_n, w_b) in enumerate(zip(self.weights, self.bias)):\n",
    "#                 print(\"{} | {} | {} | {} | {}\".format(i, prev_input, w_n, w_b, np.dot(w_n, prev_input) + w_b))\n",
    "                out_h_layer = np.dot(w_n, prev_input) + w_b\n",
    "                prev_input = [self.__activation(out) for out in out_h_layer] \n",
    "                \n",
    "            result.append(self.__activation(prev_input[0]))\n",
    "        return result\n",
    "    \n",
    "    def fit(self, train_x, train_y):\n",
    "        n_data, n_feature = train_x.shape\n",
    "        n_batch = int(n_data / self.batch_size)\n",
    "        batch_size = self.batch_size\n",
    "        \n",
    "        self.__init_neuron_weight(n_feature)\n",
    "        print(self.weights)\n",
    "        print(self.bias)\n",
    "        \n",
    "        for itr in range(self.max_epochs):\n",
    "            last_delta_weight = [np.zeros(weight.shape) for weight in self.weights]\n",
    "            last_delta_bias = [np.zeros(bias.shape) for bias in self.bias]\n",
    "            \n",
    "            for n in range(0, n_data, batch_size):\n",
    "                batch_x = train_x[n : n + batch_size]\n",
    "                batch_y = train_y[n : n + batch_size]\n",
    "                \n",
    "                # print(\"{}\".format(batch_x))\n",
    "                \n",
    "                delta_weight = [np.zeros(weight.shape) for weight in self.weights]\n",
    "                delta_bias = [np.zeros(bias.shape) for bias in self.bias]\n",
    "            \n",
    "                for x, y in zip(batch_x, batch_y):\n",
    "                    # feed forward\n",
    "                    # print(x)\n",
    "                    prev_input = x\n",
    "                    output_h_layer = []\n",
    "                    for i, (w_n, w_b) in enumerate(zip(self.weights, self.bias)):\n",
    "                        out_h_layer = np.dot(w_n, prev_input) + w_b\n",
    "                        prev_input = [self.__activation(out) for out in out_h_layer] \n",
    "                        output_h_layer.append(np.array(prev_input))\n",
    "                    \n",
    "                    # print(\"out {}\".format(output_h_layer))\n",
    "                    # backprop\n",
    "                    error_h_layer = [np.zeros(o_l.shape) for o_l in output_h_layer]\n",
    "                    error_h_layer[-1] = np.array(self.__error_derivative(output_h_layer[-1], y))\n",
    "                    for i in reversed(range((len(error_h_layer) - 1))):\n",
    "                        # print(\"{} | {} | {} | {}\".format(i, self.weights[i+1], error_h_layer[i+1].T, \"tes\"))\n",
    "                        error_h_layer[i] = (np.dot(error_h_layer[i+1].T, self.weights[i+1]) \n",
    "                                            * self.__activation_derivative(output_h_layer[i]))\n",
    "                        \n",
    "                    # print(\"err {}\".format(error_h_layer))\n",
    "                        \n",
    "                    delta_weight[0] += (self.learning_rate * np.dot(np.array([error_h_layer[0]]).T, \n",
    "                                        np.array([x])) + last_delta_weight[0]*self.momentum)\n",
    "                    delta_bias[0] += self.learning_rate * error_h_layer[0] + last_delta_bias[0] * self.momentum\n",
    "                    for i in range(1, len(self.weights)):\n",
    "                        delta_weight[i] += (self.learning_rate * np.dot(np.array([error_h_layer[i]]).T, \n",
    "                                            np.array([output_h_layer[i-1]])) + last_delta_weight[i]*self.momentum)\n",
    "                        delta_bias[i] += self.learning_rate * error_h_layer[i] + last_delta_bias[i] * self.momentum\n",
    "                        \n",
    "                    \n",
    "                # update weight\n",
    "                delta_weight = np.array(delta_weight) / len(batch_x)\n",
    "                delta_bias = np.array(delta_bias) / len(batch_x)\n",
    "                \n",
    "                self.weights -= delta_weight\n",
    "                self.bias -= delta_bias\n",
    "                \n",
    "                last_delta_weight = delta_weight\n",
    "                last_delta_bias = delta_bias\n",
    "                \n",
    "            # print(\"{} ======================================== \".format(itr))\n",
    "        \n",
    "        return self\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = NeuralNetwork(max_epochs=100, batch_size=1, neuron_hidden_layer=[2,2])\n",
    "\n",
    "train_x = np.array([[10,10],[-2,-2]])\n",
    "train_y = np.array([1,0])\n",
    "\n",
    "start_time = time.time()\n",
    "test.fit(train_x, train_y)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "print(test.predict(train_x))\n",
    "\n",
    "test.get_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
