{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, layer_size= [50,1], lr= 0.001, momentum= 0, batch_size= 5, init_seed = None, nb_epoch= 300):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.layer_size = layer_size\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.velocity = []\n",
    "        self.output_layer = []\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epoch = nb_epoch\n",
    "        \n",
    "        if init_seed != None:\n",
    "            np.random.seed(init_seed)\n",
    "            \n",
    "        for i, layer in enumerate(layer_size):\n",
    "            if i < (len(layer_size) - 1):\n",
    "                self.weights.append(np.random.rand(layer_size[i], layer_size[i+1]))\n",
    "                self.velocity.append(np.zeros((layer_size[i], layer_size[i+1])))\n",
    "            self.biases.append(np.random.rand(layer_size[i]))\n",
    "        \n",
    "    def _sigmoid(self, x, derivative= False):\n",
    "        sigmoid =  1 / (1 + np.exp(-x))\n",
    "        if derivative:\n",
    "            return sigmoid * (1 - sigmoid)\n",
    "        else:\n",
    "            return sigmoid\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x = np.array(x)\n",
    "        self.y = np.array(y)\n",
    "        self.weights = [np.random.rand(self.x.shape[1], self.layer_size[0])] + self.weights\n",
    "        self.velocity = [np.zeros((self.x.shape[1], self.layer_size[0]))] + self.velocity\n",
    "        \n",
    "        for i in range(self.nb_epoch):\n",
    "            loss = 0\n",
    "            for idx in range(0, self.x.shape[0], self.batch_size):\n",
    "                data_in = self.x[idx : min(idx + self.batch_size, self.x.shape[0])]\n",
    "                data_target = self.y[idx : min(idx + self.batch_size, self.x.shape[0])]\n",
    "                out = self._feed_forward(data_in)\n",
    "                loss += np.sum((out-data_target)**2)\n",
    "                self._backprop(data_in, data_target)\n",
    "            print(\"Epoch {}, loss: {}\".format(i, loss))\n",
    "    \n",
    "    def _feed_forward(self, input_data= []):\n",
    "        self.output_layer = []\n",
    "        in_layer = input_data\n",
    "        for weight, bias in zip(self.weights, self.biases):\n",
    "            out_layer = self._sigmoid(np.dot(in_layer, weight) + bias)\n",
    "            self.output_layer.append(out_layer)\n",
    "            \n",
    "            in_layer = out_layer\n",
    "        return out_layer\n",
    "        \n",
    "    def _backprop(self, data_in, data_target):\n",
    "        for i, w in reversed(list(enumerate(self.weights))):\n",
    "            if i == len(self.weights) - 1:\n",
    "                out_d = 2 * (data_target - self.output_layer[i]) * self._sigmoid(self.output_layer[i], derivative= True)\n",
    "                d_weight = np.dot(self.output_layer[i - 1].T, out_d) + self.velocity[i] * self.momentum\n",
    "            elif i == 0:\n",
    "                out_d = np.dot(out_d, self.weights[i + 1].T) * self._sigmoid(self.output_layer[i], derivative= True)\n",
    "                d_weight = np.dot(data_in.T, out_d) + self.velocity[i] * self.momentum\n",
    "            else:\n",
    "                out_d = np.dot(out_d, self.weights[i + 1].T) * self._sigmoid(self.output_layer[i], derivative= True)\n",
    "                d_weight = np.dot(self.output_layer[i - 1].T, out_d) + self.velocity[i] * self.momentum\n",
    "            \n",
    "            self.velocity[i] = d_weight\n",
    "        \n",
    "        for i, w in enumerate(self.weights):\n",
    "            self.weights[i] += self.velocity[i]\n",
    "            \n",
    "    \n",
    "    def predict(self, data):\n",
    "        return self._feed_forward(data)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def get_biases(self):\n",
    "        return self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 1.7573472554572893\n",
      "Epoch 1, loss: 1.3285761692853921\n",
      "Epoch 2, loss: 1.0802555026694367\n",
      "Epoch 3, loss: 1.0689068813426559\n",
      "Epoch 4, loss: 1.0646589523547962\n",
      "Epoch 5, loss: 1.0604960374867103\n",
      "Epoch 6, loss: 1.0563153576366997\n",
      "Epoch 7, loss: 1.0520561288540802\n",
      "Epoch 8, loss: 1.0476542434366976\n",
      "Epoch 9, loss: 1.0430401115370098\n",
      "Epoch 10, loss: 1.0381368478283115\n",
      "Epoch 11, loss: 1.0328581163795159\n",
      "Epoch 12, loss: 1.0271056521722193\n",
      "Epoch 13, loss: 1.0207664264736285\n",
      "Epoch 14, loss: 1.013709437621643\n",
      "Epoch 15, loss: 1.0057821365893644\n",
      "Epoch 16, loss: 0.9968065447993975\n",
      "Epoch 17, loss: 0.9865752003275927\n",
      "Epoch 18, loss: 0.9748471914019823\n",
      "Epoch 19, loss: 0.9613447199968375\n",
      "Epoch 20, loss: 0.9457509026213731\n",
      "Epoch 21, loss: 0.9277098775514775\n",
      "Epoch 22, loss: 0.9068307539150195\n",
      "Epoch 23, loss: 0.8826974845721463\n",
      "Epoch 24, loss: 0.854887286284429\n",
      "Epoch 25, loss: 0.8230005727231485\n",
      "Epoch 26, loss: 0.7867051482216159\n",
      "Epoch 27, loss: 0.7457960692675086\n",
      "Epoch 28, loss: 0.7002693977149523\n",
      "Epoch 29, loss: 0.6504024229320244\n",
      "Epoch 30, loss: 0.5968249018792281\n",
      "Epoch 31, loss: 0.5405572643288612\n",
      "Epoch 32, loss: 0.4829871557765728\n",
      "Epoch 33, loss: 0.4257622685689052\n",
      "Epoch 34, loss: 0.37060080352454816\n",
      "Epoch 35, loss: 0.3190570887085898\n",
      "Epoch 36, loss: 0.27230974037057604\n",
      "Epoch 37, loss: 0.23103897169130147\n",
      "Epoch 38, loss: 0.19542244270667158\n",
      "Epoch 39, loss: 0.16522900978716853\n",
      "Epoch 40, loss: 0.13995925549914645\n",
      "Epoch 41, loss: 0.1189828958685787\n",
      "Epoch 42, loss: 0.10164395883516747\n",
      "Epoch 43, loss: 0.08732662839338448\n",
      "Epoch 44, loss: 0.07548796648274664\n",
      "Epoch 45, loss: 0.06566804728284428\n",
      "Epoch 46, loss: 0.05748709428801689\n",
      "Epoch 47, loss: 0.050636459041434254\n",
      "Epoch 48, loss: 0.044867609260711414\n",
      "Epoch 49, loss: 0.039981340799939036\n",
      "Epoch 50, loss: 0.035818204598029396\n",
      "Epoch 51, loss: 0.03225045679934023\n",
      "Epoch 52, loss: 0.029175499999714367\n",
      "Epoch 53, loss: 0.02651063947768001\n",
      "Epoch 54, loss: 0.024188936932289497\n",
      "Epoch 55, loss: 0.022155951543302643\n",
      "Epoch 56, loss: 0.020367184959476092\n",
      "Epoch 57, loss: 0.018786078536702222\n",
      "Epoch 58, loss: 0.017382441185372417\n",
      "Epoch 59, loss: 0.01613121205136877\n",
      "Epoch 60, loss: 0.015011483435658037\n",
      "Epoch 61, loss: 0.014005726204986725\n",
      "Epoch 62, loss: 0.01309917311561296\n",
      "Epoch 63, loss: 0.012279325660020768\n",
      "Epoch 64, loss: 0.011535557882108005\n",
      "Epoch 65, loss: 0.010858796615661645\n",
      "Epoch 66, loss: 0.010241262206343225\n",
      "Epoch 67, loss: 0.009676257309529548\n",
      "Epoch 68, loss: 0.009157994070195245\n",
      "Epoch 69, loss: 0.008681452081456455\n",
      "Epoch 70, loss: 0.008242261133590547\n",
      "Epoch 71, loss: 0.007836604017651315\n",
      "Epoch 72, loss: 0.007461135622356128\n",
      "Epoch 73, loss: 0.007112915324275153\n",
      "Epoch 74, loss: 0.006789350268522903\n",
      "Epoch 75, loss: 0.006488147607439137\n",
      "Epoch 76, loss: 0.006207274136613924\n",
      "Epoch 77, loss: 0.005944922062851407\n",
      "Epoch 78, loss: 0.005699479874010623\n",
      "Epoch 79, loss: 0.005469507469004914\n",
      "Epoch 80, loss: 0.005253714857563591\n",
      "Epoch 81, loss: 0.005050943861401058\n",
      "Epoch 82, loss: 0.004860152347238411\n",
      "Epoch 83, loss: 0.004680400602400677\n",
      "Epoch 84, loss: 0.004510839529176643\n",
      "Epoch 85, loss: 0.004350700387698393\n",
      "Epoch 86, loss: 0.004199285861086816\n",
      "Epoch 87, loss: 0.004055962252850257\n",
      "Epoch 88, loss: 0.003920152656479396\n",
      "Epoch 89, loss: 0.003791330962019834\n",
      "Epoch 90, loss: 0.0036690165850632836\n",
      "Epoch 91, loss: 0.003552769820833491\n",
      "Epoch 92, loss: 0.003442187740462758\n",
      "Epoch 93, loss: 0.0033369005586549967\n",
      "Epoch 94, loss: 0.003236568412111311\n",
      "Epoch 95, loss: 0.003140878496683058\n",
      "Epoch 96, loss: 0.003049542518482695\n",
      "Epoch 97, loss: 0.002962294420343594\n",
      "Epoch 98, loss: 0.0028788883502581346\n",
      "Epoch 99, loss: 0.002799096842886821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02528667],\n",
       "       [0.02484969]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = ANN(layer_size=[4, 1], momentum= 0.005, lr= 0.01, batch_size= 4, nb_epoch= 100)\n",
    "x = [[0,1], [0,0], [1,0], [1,1]]\n",
    "y = [[0], [0], [1], [1]]\n",
    "\n",
    "\n",
    "ann.fit(x, y)\n",
    "ann.predict([[0,0], [0,0.1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
