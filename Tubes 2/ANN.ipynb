{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, layer_size= [50,1], lr= 0.001, momentum= 0, batch_size= None, init_seed = None, nb_epoch= 300):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.layer_size = layer_size\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.velocity = []\n",
    "        self.output_layer = []\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.nb_epoch = nb_epoch\n",
    "        \n",
    "        if init_seed != None:\n",
    "            np.random.seed(init_seed)\n",
    "            \n",
    "        for i, layer in enumerate(layer_size):\n",
    "            if i < (len(layer_size) - 1):\n",
    "                self.weights.append(np.random.rand(layer_size[i], layer_size[i+1]))\n",
    "                self.velocity.append(np.zeros((layer_size[i], layer_size[i+1])))\n",
    "            self.biases.append(np.random.rand(layer_size[i]))\n",
    "        \n",
    "    def _sigmoid(self, x, derivative= False):\n",
    "        sigmoid =  1 / (1 + np.exp(-x))\n",
    "        if derivative:\n",
    "            return sigmoid * (1 - sigmoid)\n",
    "        else:\n",
    "            return sigmoid\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x = np.array(x, dtype= np.float32)\n",
    "        self.y = np.array(y, dtype= np.float32)\n",
    "        if len(self.y.shape) == 1:\n",
    "            self.y = np.array([[y] for y in self.y], dtype= np.float32)\n",
    "        \n",
    "        self.weights = [np.random.rand(self.x.shape[1], self.layer_size[0])] + self.weights\n",
    "        self.velocity = [np.zeros((self.x.shape[1], self.layer_size[0]))] + self.velocity\n",
    "        if self.batch_size is None:\n",
    "            self.batch_size = self.x.shape[0]\n",
    "        \n",
    "        for i in range(self.nb_epoch):\n",
    "            loss = 0\n",
    "            indices = np.arange(self.y.shape[0])\n",
    "            np.random.shuffle(indices)            \n",
    "            self.x = self.x[indices]\n",
    "            self.y = self.y[indices]\n",
    "            \n",
    "            for idx in range(0, self.x.shape[0], self.batch_size):\n",
    "                data_in = self.x[idx : min(idx + self.batch_size, self.x.shape[0])]\n",
    "                data_target = self.y[idx : min(idx + self.batch_size, self.x.shape[0])]\n",
    "                out = self._feed_forward(data_in)\n",
    "                loss += np.sum((out-data_target)**2)\n",
    "                self._backprop(data_in, data_target)\n",
    "            print(\"Epoch {}, loss: {}\".format(i, loss))\n",
    "    \n",
    "    def _feed_forward(self, input_data= []):\n",
    "        self.output_layer = []\n",
    "        in_layer = input_data\n",
    "        for weight, bias in zip(self.weights, self.biases):\n",
    "            out_layer = self._sigmoid(np.dot(in_layer, weight) + bias)\n",
    "            self.output_layer.append(out_layer)\n",
    "            \n",
    "            in_layer = out_layer\n",
    "        return out_layer\n",
    "        \n",
    "    def _backprop(self, data_in, data_target):\n",
    "        for i, w in reversed(list(enumerate(self.weights))):\n",
    "            if i == len(self.weights) - 1:\n",
    "                out_d = 2 * (data_target - self.output_layer[i]) * self._sigmoid(self.output_layer[i], derivative= True)\n",
    "                d_weight = np.dot(self.output_layer[i - 1].T, out_d) + self.velocity[i] * self.momentum\n",
    "            elif i == 0:\n",
    "                out_d = np.dot(out_d, self.weights[i + 1].T) * self._sigmoid(self.output_layer[i], derivative= True)\n",
    "                d_weight = np.dot(data_in.T, out_d) + self.velocity[i] * self.momentum\n",
    "            else:\n",
    "                out_d = np.dot(out_d, self.weights[i + 1].T) * self._sigmoid(self.output_layer[i], derivative= True)\n",
    "                d_weight = np.dot(self.output_layer[i - 1].T, out_d) + self.velocity[i] * self.momentum\n",
    "            \n",
    "            self.velocity[i] = d_weight\n",
    "        \n",
    "        for i, w in enumerate(self.weights):\n",
    "            self.weights[i] += self.velocity[i]\n",
    "            \n",
    "    \n",
    "    def predict(self, data):\n",
    "        data = np.array(data, dtype= np.float32)\n",
    "        return self._feed_forward(data)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def get_biases(self):\n",
    "        return self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "Data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 1.6069532481795483\n",
      "Epoch 1, loss: 1.1425182415577453\n",
      "Epoch 2, loss: 1.0150783086451496\n",
      "Epoch 3, loss: 1.0112568846179064\n",
      "Epoch 4, loss: 1.0107702982449325\n",
      "Epoch 5, loss: 1.010294459933176\n",
      "Epoch 6, loss: 1.0098214653353639\n",
      "Epoch 7, loss: 1.0093511330317906\n",
      "Epoch 8, loss: 1.0088831224084278\n",
      "Epoch 9, loss: 1.0084170542893125\n",
      "Epoch 10, loss: 1.0079525379781344\n",
      "Epoch 11, loss: 1.0074891709949214\n",
      "Epoch 12, loss: 1.0070265377451246\n",
      "Epoch 13, loss: 1.0065642082072441\n",
      "Epoch 14, loss: 1.0061017365768576\n",
      "Epoch 15, loss: 1.005638659848557\n",
      "Epoch 16, loss: 1.005174496324622\n",
      "Epoch 17, loss: 1.0047087440389106\n",
      "Epoch 18, loss: 1.0042408790836173\n",
      "Epoch 19, loss: 1.003770353825593\n",
      "Epoch 20, loss: 1.0032965949977795\n",
      "Epoch 21, loss: 1.0028190016499998\n",
      "Epoch 22, loss: 1.0023369429418116\n",
      "Epoch 23, loss: 1.0018497557584007\n",
      "Epoch 24, loss: 1.0013567421284653\n",
      "Epoch 25, loss: 1.000857166420769\n",
      "Epoch 26, loss: 1.000350252293416\n",
      "Epoch 27, loss: 0.9998351793669309\n",
      "Epoch 28, loss: 0.999311079588819\n",
      "Epoch 29, loss: 0.9987770332534255\n",
      "Epoch 30, loss: 0.998232064636482\n",
      "Epoch 31, loss: 0.9976751371987085\n",
      "Epoch 32, loss: 0.9971051483070855\n",
      "Epoch 33, loss: 0.996520923415839\n",
      "Epoch 34, loss: 0.9959212096416874\n",
      "Epoch 35, loss: 0.9953046686592932\n",
      "Epoch 36, loss: 0.994669868833024\n",
      "Epoch 37, loss: 0.9940152764898325\n",
      "Epoch 38, loss: 0.9933392462251076\n",
      "Epoch 39, loss: 0.9926400101184425\n",
      "Epoch 40, loss: 0.991915665719135\n",
      "Epoch 41, loss: 0.9911641626414838\n",
      "Epoch 42, loss: 0.9903832875872133\n",
      "Epoch 43, loss: 0.9895706475860839\n",
      "Epoch 44, loss: 0.9887236512154626\n",
      "Epoch 45, loss: 0.9878394875245862\n",
      "Epoch 46, loss: 0.9869151023487733\n",
      "Epoch 47, loss: 0.9859471716520025\n",
      "Epoch 48, loss: 0.9849320714820908\n",
      "Epoch 49, loss: 0.9838658440600077\n",
      "Epoch 50, loss: 0.9827441594523321\n",
      "Epoch 51, loss: 0.9815622721920174\n",
      "Epoch 52, loss: 0.9803149721157416\n",
      "Epoch 53, loss: 0.9789965285744124\n",
      "Epoch 54, loss: 0.9776006270447579\n",
      "Epoch 55, loss: 0.9761202970222855\n",
      "Epoch 56, loss: 0.9745478299070077\n",
      "Epoch 57, loss: 0.9728746854011612\n",
      "Epoch 58, loss: 0.9710913847208873\n",
      "Epoch 59, loss: 0.9691873886804001\n",
      "Epoch 60, loss: 0.9671509584376405\n",
      "Epoch 61, loss: 0.964968996396832\n",
      "Epoch 62, loss: 0.9626268644509035\n",
      "Epoch 63, loss: 0.9601081764254318\n",
      "Epoch 64, loss: 0.9573945612726932\n",
      "Epoch 65, loss: 0.9544653932874134\n",
      "Epoch 66, loss: 0.9512974854179004\n",
      "Epoch 67, loss: 0.9478647416923996\n",
      "Epoch 68, loss: 0.94413776496647\n",
      "Epoch 69, loss: 0.9400834167612315\n",
      "Epoch 70, loss: 0.9356643271008642\n",
      "Epoch 71, loss: 0.9308383542442953\n",
      "Epoch 72, loss: 0.925557997414383\n",
      "Epoch 73, loss: 0.9197697705570709\n",
      "Epoch 74, loss: 0.9134135524601852\n",
      "Epoch 75, loss: 0.9064219390352963\n",
      "Epoch 76, loss: 0.8987196381727838\n",
      "Epoch 77, loss: 0.890222967359173\n",
      "Epoch 78, loss: 0.8808395401585666\n",
      "Epoch 79, loss: 0.8704682602756109\n",
      "Epoch 80, loss: 0.8589997808632029\n",
      "Epoch 81, loss: 0.8463176298363938\n",
      "Epoch 82, loss: 0.832300243999752\n",
      "Epoch 83, loss: 0.8168241860528294\n",
      "Epoch 84, loss: 0.799768823291953\n",
      "Epoch 85, loss: 0.781022702774138\n",
      "Epoch 86, loss: 0.7604917375742951\n",
      "Epoch 87, loss: 0.7381090960163615\n",
      "Epoch 88, loss: 0.7138463461282643\n",
      "Epoch 89, loss: 0.6877249663403147\n",
      "Epoch 90, loss: 0.6598268542565567\n",
      "Epoch 91, loss: 0.6303020703261026\n",
      "Epoch 92, loss: 0.5993719098385899\n",
      "Epoch 93, loss: 0.5673256693645236\n",
      "Epoch 94, loss: 0.5345102457390012\n",
      "Epoch 95, loss: 0.5013129010260495\n",
      "Epoch 96, loss: 0.46813888342224297\n",
      "Epoch 97, loss: 0.4353867260457211\n",
      "Epoch 98, loss: 0.4034245879965125\n",
      "Epoch 99, loss: 0.37257076986711757\n",
      "Epoch 100, loss: 0.34308060899475473\n",
      "Epoch 101, loss: 0.315140643932176\n",
      "Epoch 102, loss: 0.2888696257259587\n",
      "Epoch 103, loss: 0.2643249684091525\n",
      "Epoch 104, loss: 0.2415127287628259\n",
      "Epoch 105, loss: 0.22039916915505572\n",
      "Epoch 106, loss: 0.20092225752924125\n",
      "Epoch 107, loss: 0.18300193092276068\n",
      "Epoch 108, loss: 0.16654845191067105\n",
      "Epoch 109, loss: 0.151468627884929\n",
      "Epoch 110, loss: 0.13766999517405062\n",
      "Epoch 111, loss: 0.12506328235290964\n",
      "Epoch 112, loss: 0.11356356940907136\n",
      "Epoch 113, loss: 0.10309057269803405\n",
      "Epoch 114, loss: 0.09356843592068335\n",
      "Epoch 115, loss: 0.0849253216511096\n",
      "Epoch 116, loss: 0.077093000554996\n",
      "Epoch 117, loss: 0.07000654521437953\n",
      "Epoch 118, loss: 0.06360416415707637\n",
      "Epoch 119, loss: 0.05782716389367924\n",
      "Epoch 120, loss: 0.05262000137988379\n",
      "Epoch 121, loss: 0.047930381690616586\n",
      "Epoch 122, loss: 0.04370935982083183\n",
      "Epoch 123, loss: 0.03991141576479253\n",
      "Epoch 124, loss: 0.036494484006703846\n",
      "Epoch 125, loss: 0.03341992948257826\n",
      "Epoch 126, loss: 0.030652470549327623\n",
      "Epoch 127, loss: 0.028160055153958252\n",
      "Epoch 128, loss: 0.02591369949116496\n",
      "Epoch 129, loss: 0.023887299523296508\n",
      "Epoch 130, loss: 0.022057425430280844\n",
      "Epoch 131, loss: 0.02040310791239063\n",
      "Epoch 132, loss: 0.018905623719439105\n",
      "Epoch 133, loss: 0.017548286130842965\n",
      "Epoch 134, loss: 0.016316244553519644\n",
      "Epoch 135, loss: 0.015196296042570046\n",
      "Epoch 136, loss: 0.01417671042510368\n",
      "Epoch 137, loss: 0.013247069822732542\n",
      "Epoch 138, loss: 0.012398122702844183\n",
      "Epoch 139, loss: 0.011621652111603115\n",
      "Epoch 140, loss: 0.010910357418423808\n",
      "Epoch 141, loss: 0.010257748699509683\n",
      "Epoch 142, loss: 0.009658052777780911\n",
      "Epoch 143, loss: 0.009106129893599123\n",
      "Epoch 144, loss: 0.008597399985320404\n",
      "Epoch 145, loss: 0.008127777595378133\n",
      "Epoch 146, loss: 0.007693614474539879\n",
      "Epoch 147, loss: 0.007291649025489468\n",
      "Epoch 148, loss: 0.00691896180069838\n",
      "Epoch 149, loss: 0.006572936344309598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02588839],\n",
       "       [0.02736542]])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = ANN(layer_size=[4, 4, 1], momentum= 0.005, lr= 0.01, batch_size= 4, nb_epoch= 150)\n",
    "x = [[0,1], [0,0], [1,0], [1,1]]\n",
    "y = [[0], [0], [1], [1]]\n",
    "\n",
    "\n",
    "ann.fit(x, y)\n",
    "ann.predict([[0,0], [0,0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "Data weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Loading data\n",
    "data, meta = arff.loadarff('weather.arff')\n",
    "data_x = [list(d)[:4] for d in data]\n",
    "data_y = [[d[4]] for d in data]\n",
    "\n",
    "# encoding data\n",
    "le = LabelEncoder()\n",
    "data_y = le.fit_transform(data_y)\n",
    "\n",
    "ct = ColumnTransformer([('ohe', OneHotEncoder(), [0, 3]),], remainder= 'passthrough')\n",
    "data_x = ct.fit_transform(data_x)\n",
    "\n",
    "# splitting data into train and test data\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 4.577117681769888\n",
      "Epoch 1, loss: 2.77523638612105\n",
      "Epoch 2, loss: 2.77523638612105\n",
      "Epoch 3, loss: 2.77523638612105\n",
      "Epoch 4, loss: 2.77523638612105\n",
      "Epoch 5, loss: 2.77523638612105\n",
      "Epoch 6, loss: 2.77523638612105\n",
      "Epoch 7, loss: 2.77523638612105\n",
      "Epoch 8, loss: 2.77523638612105\n",
      "Epoch 9, loss: 2.77523638612105\n",
      "Epoch 10, loss: 2.77523638612105\n",
      "Epoch 11, loss: 2.77523638612105\n",
      "Epoch 12, loss: 2.77523638612105\n",
      "Epoch 13, loss: 2.77523638612105\n",
      "Epoch 14, loss: 2.77523638612105\n",
      "Epoch 15, loss: 2.77523638612105\n",
      "Epoch 16, loss: 2.77523638612105\n",
      "Epoch 17, loss: 2.77523638612105\n",
      "Epoch 18, loss: 2.77523638612105\n",
      "Epoch 19, loss: 2.77523638612105\n",
      "Epoch 20, loss: 2.77523638612105\n",
      "Epoch 21, loss: 2.77523638612105\n",
      "Epoch 22, loss: 2.77523638612105\n",
      "Epoch 23, loss: 2.77523638612105\n",
      "Epoch 24, loss: 2.77523638612105\n",
      "Epoch 25, loss: 2.77523638612105\n",
      "Epoch 26, loss: 2.77523638612105\n",
      "Epoch 27, loss: 2.77523638612105\n",
      "Epoch 28, loss: 2.77523638612105\n",
      "Epoch 29, loss: 2.77523638612105\n",
      "Epoch 30, loss: 2.77523638612105\n",
      "Epoch 31, loss: 2.77523638612105\n",
      "Epoch 32, loss: 2.77523638612105\n",
      "Epoch 33, loss: 2.77523638612105\n",
      "Epoch 34, loss: 2.77523638612105\n",
      "Epoch 35, loss: 2.77523638612105\n",
      "Epoch 36, loss: 2.77523638612105\n",
      "Epoch 37, loss: 2.77523638612105\n",
      "Epoch 38, loss: 2.77523638612105\n",
      "Epoch 39, loss: 2.77523638612105\n",
      "Epoch 40, loss: 2.77523638612105\n",
      "Epoch 41, loss: 2.77523638612105\n",
      "Epoch 42, loss: 2.77523638612105\n",
      "Epoch 43, loss: 2.77523638612105\n",
      "Epoch 44, loss: 2.77523638612105\n",
      "Epoch 45, loss: 2.77523638612105\n",
      "Epoch 46, loss: 2.77523638612105\n",
      "Epoch 47, loss: 2.77523638612105\n",
      "Epoch 48, loss: 2.77523638612105\n",
      "Epoch 49, loss: 2.77523638612105\n",
      "Epoch 50, loss: 2.77523638612105\n",
      "Epoch 51, loss: 2.77523638612105\n",
      "Epoch 52, loss: 2.77523638612105\n",
      "Epoch 53, loss: 2.77523638612105\n",
      "Epoch 54, loss: 2.77523638612105\n",
      "Epoch 55, loss: 2.77523638612105\n",
      "Epoch 56, loss: 2.77523638612105\n",
      "Epoch 57, loss: 2.77523638612105\n",
      "Epoch 58, loss: 2.77523638612105\n",
      "Epoch 59, loss: 2.77523638612105\n",
      "Epoch 60, loss: 2.77523638612105\n",
      "Epoch 61, loss: 2.77523638612105\n",
      "Epoch 62, loss: 2.77523638612105\n",
      "Epoch 63, loss: 2.77523638612105\n",
      "Epoch 64, loss: 2.77523638612105\n",
      "Epoch 65, loss: 2.77523638612105\n",
      "Epoch 66, loss: 2.77523638612105\n",
      "Epoch 67, loss: 2.77523638612105\n",
      "Epoch 68, loss: 2.77523638612105\n",
      "Epoch 69, loss: 2.77523638612105\n",
      "Epoch 70, loss: 2.77523638612105\n",
      "Epoch 71, loss: 2.77523638612105\n",
      "Epoch 72, loss: 2.77523638612105\n",
      "Epoch 73, loss: 2.77523638612105\n",
      "Epoch 74, loss: 2.77523638612105\n",
      "Epoch 75, loss: 2.77523638612105\n",
      "Epoch 76, loss: 2.77523638612105\n",
      "Epoch 77, loss: 2.77523638612105\n",
      "Epoch 78, loss: 2.77523638612105\n",
      "Epoch 79, loss: 2.77523638612105\n",
      "Epoch 80, loss: 2.77523638612105\n",
      "Epoch 81, loss: 2.77523638612105\n",
      "Epoch 82, loss: 2.77523638612105\n",
      "Epoch 83, loss: 2.77523638612105\n",
      "Epoch 84, loss: 2.77523638612105\n",
      "Epoch 85, loss: 2.77523638612105\n",
      "Epoch 86, loss: 2.77523638612105\n",
      "Epoch 87, loss: 2.77523638612105\n",
      "Epoch 88, loss: 2.77523638612105\n",
      "Epoch 89, loss: 2.77523638612105\n",
      "Epoch 90, loss: 2.77523638612105\n",
      "Epoch 91, loss: 2.77523638612105\n",
      "Epoch 92, loss: 2.77523638612105\n",
      "Epoch 93, loss: 2.77523638612105\n",
      "Epoch 94, loss: 2.77523638612105\n",
      "Epoch 95, loss: 2.77523638612105\n",
      "Epoch 96, loss: 2.77523638612105\n",
      "Epoch 97, loss: 2.77523638612105\n",
      "Epoch 98, loss: 2.77523638612105\n",
      "Epoch 99, loss: 2.77523638612105\n",
      "Epoch 100, loss: 2.77523638612105\n",
      "Epoch 101, loss: 2.77523638612105\n",
      "Epoch 102, loss: 2.77523638612105\n",
      "Epoch 103, loss: 2.77523638612105\n",
      "Epoch 104, loss: 2.77523638612105\n",
      "Epoch 105, loss: 2.77523638612105\n",
      "Epoch 106, loss: 2.77523638612105\n",
      "Epoch 107, loss: 2.77523638612105\n",
      "Epoch 108, loss: 2.77523638612105\n",
      "Epoch 109, loss: 2.77523638612105\n",
      "Epoch 110, loss: 2.77523638612105\n",
      "Epoch 111, loss: 2.77523638612105\n",
      "Epoch 112, loss: 2.77523638612105\n",
      "Epoch 113, loss: 2.77523638612105\n",
      "Epoch 114, loss: 2.77523638612105\n",
      "Epoch 115, loss: 2.77523638612105\n",
      "Epoch 116, loss: 2.77523638612105\n",
      "Epoch 117, loss: 2.77523638612105\n",
      "Epoch 118, loss: 2.77523638612105\n",
      "Epoch 119, loss: 2.77523638612105\n",
      "Epoch 120, loss: 2.77523638612105\n",
      "Epoch 121, loss: 2.77523638612105\n",
      "Epoch 122, loss: 2.77523638612105\n",
      "Epoch 123, loss: 2.77523638612105\n",
      "Epoch 124, loss: 2.77523638612105\n",
      "Epoch 125, loss: 2.77523638612105\n",
      "Epoch 126, loss: 2.77523638612105\n",
      "Epoch 127, loss: 2.77523638612105\n",
      "Epoch 128, loss: 2.77523638612105\n",
      "Epoch 129, loss: 2.77523638612105\n",
      "Epoch 130, loss: 2.77523638612105\n",
      "Epoch 131, loss: 2.77523638612105\n",
      "Epoch 132, loss: 2.77523638612105\n",
      "Epoch 133, loss: 2.77523638612105\n",
      "Epoch 134, loss: 2.77523638612105\n",
      "Epoch 135, loss: 2.77523638612105\n",
      "Epoch 136, loss: 2.77523638612105\n",
      "Epoch 137, loss: 2.77523638612105\n",
      "Epoch 138, loss: 2.77523638612105\n",
      "Epoch 139, loss: 2.77523638612105\n",
      "Epoch 140, loss: 2.77523638612105\n",
      "Epoch 141, loss: 2.77523638612105\n",
      "Epoch 142, loss: 2.77523638612105\n",
      "Epoch 143, loss: 2.77523638612105\n",
      "Epoch 144, loss: 2.77523638612105\n",
      "Epoch 145, loss: 2.77523638612105\n",
      "Epoch 146, loss: 2.77523638612105\n",
      "Epoch 147, loss: 2.77523638612105\n",
      "Epoch 148, loss: 2.77523638612105\n",
      "Epoch 149, loss: 2.77523638612105\n",
      "Epoch 150, loss: 2.77523638612105\n",
      "Epoch 151, loss: 2.77523638612105\n",
      "Epoch 152, loss: 2.77523638612105\n",
      "Epoch 153, loss: 2.77523638612105\n",
      "Epoch 154, loss: 2.77523638612105\n",
      "Epoch 155, loss: 2.77523638612105\n",
      "Epoch 156, loss: 2.77523638612105\n",
      "Epoch 157, loss: 2.77523638612105\n",
      "Epoch 158, loss: 2.77523638612105\n",
      "Epoch 159, loss: 2.77523638612105\n",
      "Epoch 160, loss: 2.77523638612105\n",
      "Epoch 161, loss: 2.77523638612105\n",
      "Epoch 162, loss: 2.77523638612105\n",
      "Epoch 163, loss: 2.77523638612105\n",
      "Epoch 164, loss: 2.77523638612105\n",
      "Epoch 165, loss: 2.77523638612105\n",
      "Epoch 166, loss: 2.77523638612105\n",
      "Epoch 167, loss: 2.77523638612105\n",
      "Epoch 168, loss: 2.77523638612105\n",
      "Epoch 169, loss: 2.77523638612105\n",
      "Epoch 170, loss: 2.77523638612105\n",
      "Epoch 171, loss: 2.77523638612105\n",
      "Epoch 172, loss: 2.77523638612105\n",
      "Epoch 173, loss: 2.77523638612105\n",
      "Epoch 174, loss: 2.77523638612105\n",
      "Epoch 175, loss: 2.77523638612105\n",
      "Epoch 176, loss: 2.77523638612105\n",
      "Epoch 177, loss: 2.77523638612105\n",
      "Epoch 178, loss: 2.77523638612105\n",
      "Epoch 179, loss: 2.77523638612105\n",
      "Epoch 180, loss: 2.77523638612105\n",
      "Epoch 181, loss: 2.77523638612105\n",
      "Epoch 182, loss: 2.77523638612105\n",
      "Epoch 183, loss: 2.77523638612105\n",
      "Epoch 184, loss: 2.77523638612105\n",
      "Epoch 185, loss: 2.77523638612105\n",
      "Epoch 186, loss: 2.77523638612105\n",
      "Epoch 187, loss: 2.77523638612105\n",
      "Epoch 188, loss: 2.77523638612105\n",
      "Epoch 189, loss: 2.77523638612105\n",
      "Epoch 190, loss: 2.77523638612105\n",
      "Epoch 191, loss: 2.77523638612105\n",
      "Epoch 192, loss: 2.77523638612105\n",
      "Epoch 193, loss: 2.77523638612105\n",
      "Epoch 194, loss: 2.77523638612105\n",
      "Epoch 195, loss: 2.77523638612105\n",
      "Epoch 196, loss: 2.77523638612105\n",
      "Epoch 197, loss: 2.77523638612105\n",
      "Epoch 198, loss: 2.77523638612105\n",
      "Epoch 199, loss: 2.77523638612105\n",
      "Epoch 200, loss: 2.77523638612105\n",
      "Epoch 201, loss: 2.77523638612105\n",
      "Epoch 202, loss: 2.77523638612105\n",
      "Epoch 203, loss: 2.77523638612105\n",
      "Epoch 204, loss: 2.77523638612105\n",
      "Epoch 205, loss: 2.77523638612105\n",
      "Epoch 206, loss: 2.77523638612105\n",
      "Epoch 207, loss: 2.77523638612105\n",
      "Epoch 208, loss: 2.77523638612105\n",
      "Epoch 209, loss: 2.77523638612105\n",
      "Epoch 210, loss: 2.77523638612105\n",
      "Epoch 211, loss: 2.77523638612105\n",
      "Epoch 212, loss: 2.77523638612105\n",
      "Epoch 213, loss: 2.77523638612105\n",
      "Epoch 214, loss: 2.77523638612105\n",
      "Epoch 215, loss: 2.77523638612105\n",
      "Epoch 216, loss: 2.77523638612105\n",
      "Epoch 217, loss: 2.77523638612105\n",
      "Epoch 218, loss: 2.77523638612105\n",
      "Epoch 219, loss: 2.77523638612105\n",
      "Epoch 220, loss: 2.77523638612105\n",
      "Epoch 221, loss: 2.77523638612105\n",
      "Epoch 222, loss: 2.77523638612105\n",
      "Epoch 223, loss: 2.77523638612105\n",
      "Epoch 224, loss: 2.77523638612105\n",
      "Epoch 225, loss: 2.77523638612105\n",
      "Epoch 226, loss: 2.77523638612105\n",
      "Epoch 227, loss: 2.77523638612105\n",
      "Epoch 228, loss: 2.77523638612105\n",
      "Epoch 229, loss: 2.77523638612105\n",
      "Epoch 230, loss: 2.77523638612105\n",
      "Epoch 231, loss: 2.77523638612105\n",
      "Epoch 232, loss: 2.77523638612105\n",
      "Epoch 233, loss: 2.77523638612105\n",
      "Epoch 234, loss: 2.77523638612105\n",
      "Epoch 235, loss: 2.77523638612105\n",
      "Epoch 236, loss: 2.77523638612105\n",
      "Epoch 237, loss: 2.77523638612105\n",
      "Epoch 238, loss: 2.77523638612105\n",
      "Epoch 239, loss: 2.77523638612105\n",
      "Epoch 240, loss: 2.77523638612105\n",
      "Epoch 241, loss: 2.77523638612105\n",
      "Epoch 242, loss: 2.77523638612105\n",
      "Epoch 243, loss: 2.77523638612105\n",
      "Epoch 244, loss: 2.77523638612105\n",
      "Epoch 245, loss: 2.77523638612105\n",
      "Epoch 246, loss: 2.77523638612105\n",
      "Epoch 247, loss: 2.77523638612105\n",
      "Epoch 248, loss: 2.77523638612105\n",
      "Epoch 249, loss: 2.77523638612105\n",
      "Epoch 250, loss: 2.77523638612105\n",
      "Epoch 251, loss: 2.77523638612105\n",
      "Epoch 252, loss: 2.77523638612105\n",
      "Epoch 253, loss: 2.77523638612105\n",
      "Epoch 254, loss: 2.77523638612105\n",
      "Epoch 255, loss: 2.77523638612105\n",
      "Epoch 256, loss: 2.77523638612105\n",
      "Epoch 257, loss: 2.77523638612105\n",
      "Epoch 258, loss: 2.77523638612105\n",
      "Epoch 259, loss: 2.77523638612105\n",
      "Epoch 260, loss: 2.77523638612105\n",
      "Epoch 261, loss: 2.77523638612105\n",
      "Epoch 262, loss: 2.77523638612105\n",
      "Epoch 263, loss: 2.77523638612105\n",
      "Epoch 264, loss: 2.77523638612105\n",
      "Epoch 265, loss: 2.77523638612105\n",
      "Epoch 266, loss: 2.77523638612105\n",
      "Epoch 267, loss: 2.77523638612105\n",
      "Epoch 268, loss: 2.77523638612105\n",
      "Epoch 269, loss: 2.77523638612105\n",
      "Epoch 270, loss: 2.77523638612105\n",
      "Epoch 271, loss: 2.77523638612105\n",
      "Epoch 272, loss: 2.77523638612105\n",
      "Epoch 273, loss: 2.77523638612105\n",
      "Epoch 274, loss: 2.77523638612105\n",
      "Epoch 275, loss: 2.77523638612105\n",
      "Epoch 276, loss: 2.77523638612105\n",
      "Epoch 277, loss: 2.77523638612105\n",
      "Epoch 278, loss: 2.77523638612105\n",
      "Epoch 279, loss: 2.77523638612105\n",
      "Epoch 280, loss: 2.77523638612105\n",
      "Epoch 281, loss: 2.77523638612105\n",
      "Epoch 282, loss: 2.77523638612105\n",
      "Epoch 283, loss: 2.77523638612105\n",
      "Epoch 284, loss: 2.77523638612105\n",
      "Epoch 285, loss: 2.77523638612105\n",
      "Epoch 286, loss: 2.77523638612105\n",
      "Epoch 287, loss: 2.77523638612105\n",
      "Epoch 288, loss: 2.77523638612105\n",
      "Epoch 289, loss: 2.77523638612105\n",
      "Epoch 290, loss: 2.77523638612105\n",
      "Epoch 291, loss: 2.77523638612105\n",
      "Epoch 292, loss: 2.77523638612105\n",
      "Epoch 293, loss: 2.77523638612105\n",
      "Epoch 294, loss: 2.77523638612105\n",
      "Epoch 295, loss: 2.77523638612105\n",
      "Epoch 296, loss: 2.77523638612105\n",
      "Epoch 297, loss: 2.77523638612105\n",
      "Epoch 298, loss: 2.77523638612105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299, loss: 2.77523638612105\n",
      "Prediction: [[0.57154844]\n",
      " [0.57154844]]\n",
      "Target: [1 0]\n"
     ]
    }
   ],
   "source": [
    "ann = ANN(layer_size=[64, 1], momentum= 0, lr= 0.025, nb_epoch= 300, batch_size= 8)\n",
    "ann.fit(train_x, train_y)\n",
    "pred = ann.predict(test_x)\n",
    "\n",
    "print(\"Prediction: {}\".format(pred))\n",
    "print(\"Target: {}\".format(test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
